[["index.html", "GOVT 8002 Shared Working Book Spring 2024 1 Introduction 1.1 Purpose of This Site 1.2 Loading Data", " GOVT 8002 Shared Working Book Spring 2024 Benjamin Reese | bfr11@georgetown.edu 1 Introduction 1.1 Purpose of This Site The goal of this site is to have a place where you can look for examples of code. Think of this site as a log of all of the topics we cover in class and in lab sessions. I will make updates throughout the semester as we cover more advanced material. I hope this proves to be a helpful and beneficial resource and offer an easier to access format than the files posted to Canvas. If you are interested, I am happy to add you to the GitHub and you can make some direct edits to add your code! Let me know if any of this code doesn’t run correctly or if you have any questions or issues! 1.2 Loading Data The easiest way to load data into R and ensure you have the correct file path is to create a folder on your computer for each assignment and place the datasets directly into that folder. Create a folder on your computer for each new analysis Download your Data and move the file to your newly created folder Then open RStudio Click the project button in the top right corner Click new project Click existing directory Click browse and find the folder that you created Click create project Once your new project opens, click the blank page with a green plus sign icon in the top left corner under the file option Click R script to open a new script You should also be able to see your data file in the bottom right window of RStudio, click the file and follow the options depending on the file type Once your data is imported into R, the code that R automatically ran will be in the console window on the bottom left, copy and paste it to your fresh R script For example, in Lab 1, my code looked like: read_excel(\"USstates.xlsx\") Run this copy and pasted line of code whenever you open the R Project and you will never have to worry about complicated file pathing commands I recommend using the assignment operator &lt;- to give your dataset a short and simple name like df, dta, or, if you are working with multiple datasets, name each something short and descriptive "],["lab-i-tidyverse.html", "2 Lab I: Tidyverse 2.1 Join the data sets. 2.2 Create and add the following four variables to your dataframe: density (based on sqMiles and pop2019), deaths per capita (based on new_death), cases per capita (based on new_case) and vaccinated percent (based on Series_Complete_12PlusPop_Pct). 2.3 Estimate three regression models with deaths per capita on your selected day as the dependent variable. 2.4 Assess specific vaccines", " 2 Lab I: Tidyverse 2.0.1 Preparation ## Packages library(tidyverse) library(dplyr) library(readxl) # Package to read Excel data library(stargazer) 2.1 Join the data sets. Join the cases and vaccination data by date and state. Case data: United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv Vaccination data: COVID-19_Vaccinations_in_the_United_States_Jurisdiction.csv Other state variables: USstates.xlsx Add the USstates.xlsx data and limit your dataframe to the states listed in USstates.xlsx. How do you know if your merge was successful? ANSWER: Looking at the dataframe is useful. Look at individual states - do the variables that are supposed to be the same (e.g. population) the same over the time period? Do the case numbers look reasonable? ## Load data # https://data.cdc.gov/Case-Surveillance/United-States-COVID-19-Cases-and-Deaths-by-State-o/9mfq-cb36 cases &lt;- read.csv(&quot;Data/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv&quot;) # Vax data # &quot;COVID-19 Vaccinations in the United States,Jurisdiction&quot; # https://data.cdc.gov/Vaccinations/COVID-19-Vaccinations-in-the-United-States-Jurisdi/unsk-b7fc vax &lt;- read.csv(&quot;Data/COVID-19_Vaccinations_in_the_United_States_Jurisdiction.csv&quot;) df &lt;- cases %&gt;% left_join(vax, by = c(&quot;submission_date&quot; = &quot;Date&quot;, &quot;state&quot; = &quot;Location&quot;)) # Add state variables stFacts &lt;- read_excel(&quot;Data/USstates.xlsx&quot;, sheet = &quot;data&quot;) %&gt;% dplyr::rename(&quot;trump2020Pct&quot; = &quot;trump2020_percent&quot;) df &lt;- df %&gt;% filter(state %in% stFacts$stateAbbr == 1) %&gt;% left_join(stFacts, by = c(&quot;state&quot; = &quot;stateAbbr&quot;)) 2.2 Create and add the following four variables to your dataframe: density (based on sqMiles and pop2019), deaths per capita (based on new_death), cases per capita (based on new_case) and vaccinated percent (based on Series_Complete_12PlusPop_Pct). For a specific day (based on submission_date), show the top five states ranked by deaths per capita and calculate the average vaccinated per capita and the mean, minimum and maximum deaths per capita. df &lt;- df %&gt;% mutate(&quot;deathsPC&quot; = 100000*new_death/pop2019, &quot;casesPC&quot; = 100000*new_case/pop2019, &quot;vaxedPct&quot; = Series_Complete_12PlusPop_Pct/100, &quot;density&quot; = pop2019/(1000000*sqMiles)) # Pick a day DATE &lt;- &quot;10/18/2021&quot; df_day &lt;- df %&gt;% filter(submission_date == DATE) %&gt;% arrange(desc(deathsPC)) # Look at data (check for negative deaths etc) df_day %&gt;% dplyr::select(submission_date, state, new_case, new_death, deathsPC, casesPC) %&gt;% slice(1:5) ## submission_date state new_case new_death deathsPC casesPC ## 1 10/18/2021 ID 1290 42 2.4 72 ## 2 10/18/2021 AK 691 16 2.2 94 ## 3 10/18/2021 MT 426 19 1.8 40 ## 4 10/18/2021 AL 961 84 1.7 20 ## 5 10/18/2021 WV 689 26 1.5 38 df_day %&gt;% dplyr::summarize(meanVaxedPct = mean(vaxedPct), meanDeath = mean(deathsPC), minDeath = min(deathsPC), maxDeath = max(deathsPC)) ## meanVaxedPct meanDeath minDeath maxDeath ## 1 0.64 0.46 0 2.4 2.3 Estimate three regression models with deaths per capita on your selected day as the dependent variable. Your first model will have only Trump 2020 percent as an independent variable. Your second model will add vaccinated percent as an independent variable. Your third model will add density. Before you estimate the models, write down your expectations about what will happen in these models. ANSWER: See table below. The key idea here is omitted variable bias. If we only have Trump percent, then vaccination rates are omitted. For October 1st, 2021, the Trump variable becomes insignificant when vaccination rates are included. One complication, for which I suspect there is no clear answer, is whether vaccination rates are a post-treatment variable. What do you think? ## Models ols.1 &lt;- lm(deathsPC ~ trump2020Pct, data = df_day) summary(ols.1) ## ## Call: ## lm(formula = deathsPC ~ trump2020Pct, data = df_day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8697 -0.2935 -0.1092 0.0997 1.6572 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.519 0.315 -1.65 0.1058 ## trump2020Pct 1.987 0.622 3.19 0.0025 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.53 on 49 degrees of freedom ## Multiple R-squared: 0.172, Adjusted R-squared: 0.155 ## F-statistic: 10.2 on 1 and 49 DF, p-value: 0.00246 ols.2 &lt;- lm(deathsPC ~ trump2020Pct + vaxedPct, data = df_day) summary(ols.2) ## ## Call: ## lm(formula = deathsPC ~ trump2020Pct + vaxedPct, data = df_day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8504 -0.3076 -0.0117 0.1775 1.6685 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.029 0.726 2.79 0.00746 ** ## trump2020Pct 0.219 0.721 0.30 0.76236 ## vaxedPct -2.627 0.691 -3.80 0.00041 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.47 on 48 degrees of freedom ## Multiple R-squared: 0.364, Adjusted R-squared: 0.337 ## F-statistic: 13.7 on 2 and 48 DF, p-value: 1.94e-05 ols.3 &lt;- lm(deathsPC ~ trump2020Pct + vaxedPct + density, data = df_day) summary(ols.3) ## ## Call: ## lm(formula = deathsPC ~ trump2020Pct + vaxedPct + density, data = df_day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8395 -0.3063 -0.0134 0.1699 1.6674 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.110 0.890 2.37 0.02194 * ## trump2020Pct 0.120 0.956 0.13 0.90028 ## vaxedPct -2.671 0.751 -3.56 0.00086 *** ## density -8.772 54.952 -0.16 0.87386 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.47 on 47 degrees of freedom ## Multiple R-squared: 0.364, Adjusted R-squared: 0.323 ## F-statistic: 8.97 on 3 and 47 DF, p-value: 8.33e-05 2.4 Assess specific vaccines Create and add vaccinated percent by state for each of the Pfizer, Modern and Janssen (which is Johnson and Johnson) vaccines. Use pop2019 for population Use Series_Complete_Moderna_18Plus, Series_Complete_Janssen_18Plus and Series_Complete_Pfizer_18Plus for the vaccination totals. Estimate a model in which deaths per capita is a function of all three vaccination rates. Explain what the results mean, especially in light of the results above for overall vaccination results. Explain how one would compare the efficacy of the individual vaccines (e.g., whether the Moderna vaccine works better than the Johnson and Johnson vaccine). ANSWER: The major issue here is multicollinearity. For October 1, 2021, the overall vaccination rate was statistically significant (see models 1 - 3 in the table) yet each of the different vaccines was insignificant (see model 4). The vaccines are multicollinear (which can be assessed with an auxilliary regression) so the loss of power is not terribly surprising. An F-test whether all the specific vaccine variables equal zero is reported for model 4 and, broadly consistent with the results in models 1 -3 we can say that we can reject the null that all individual vaccines have zero effect. To assess whether Moderna is better than Johnson and Johnson we would do an F-test where our restricted equation adds those two vaccines. If doing so causes a substantial reduction in fit we would reject the null that the effects of those two vaccines are equal. (And we would want to take such results with a grain of salt given endogeneity in which states have which vaccines.) df &lt;- df %&gt;% mutate(&quot;vaxedPctModerna&quot; = Series_Complete_Moderna_18Plus/(100*pop2019), &quot;vaxedPctJans&quot; = Series_Complete_Janssen_18Plus/(100*pop2019), &quot;vaxedPctPfizer&quot; = Series_Complete_Pfizer_18Plus/(100*pop2019)) df_day &lt;- df %&gt;% filter(submission_date == DATE) ols.4 &lt;- lm(deathsPC ~ vaxedPctJans + vaxedPctModerna + vaxedPctPfizer, data = df_day) summary(ols.4) ## ## Call: ## lm(formula = deathsPC ~ vaxedPctJans + vaxedPctModerna + vaxedPctPfizer, ## data = df_day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0240 -0.3373 -0.0304 0.1689 1.5567 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.393 0.653 3.67 0.00062 *** ## vaxedPctJans 469.840 1077.009 0.44 0.66466 ## vaxedPctModerna -156.194 541.375 -0.29 0.77422 ## vaxedPctPfizer -676.480 305.101 -2.22 0.03148 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.52 on 47 degrees of freedom ## Multiple R-squared: 0.228, Adjusted R-squared: 0.179 ## F-statistic: 4.63 on 3 and 47 DF, p-value: 0.00643 stargazer(ols.1, ols.2, ols.3, ols.4, type = &quot;html&quot;, # FOR PDF - the &quot;asis&quot; above makes it work keep.stat = c(&quot;n&quot;,&quot;ser&quot;, &quot;rsq&quot;, &quot;f&quot;), report = &quot;vcst&quot;, column.labels = c(&quot;Model 1&quot;, &quot;Model 2&quot;, &quot;Model 3&quot;, &quot;Model 4&quot;), digits = 3, dep.var.caption = &quot;&quot;, dep.var.labels.include = FALSE) Model 1 Model 2 Model 3 Model 4 (1) (2) (3) (4) trump2020Pct 2.000 0.220 0.120 (0.620) (0.720) (0.960) t = 3.200 t = 0.300 t = 0.130 vaxedPct -2.600 -2.700 (0.690) (0.750) t = -3.800 t = -3.600 density -8.800 (55.000) t = -0.160 vaxedPctJans 470.000 (1,077.000) t = 0.440 vaxedPctModerna -156.000 (541.000) t = -0.290 vaxedPctPfizer -676.000 (305.000) t = -2.200 Constant -0.520 2.000 2.100 2.400 (0.320) (0.730) (0.890) (0.650) t = -1.600 t = 2.800 t = 2.400 t = 3.700 Observations 51 51 51 51 R2 0.170 0.360 0.360 0.230 Residual Std. Error 0.530 (df = 49) 0.470 (df = 48) 0.470 (df = 47) 0.520 (df = 47) F Statistic 10.000*** (df = 1; 49) 14.000*** (df = 2; 48) 9.000*** (df = 3; 47) 4.600*** (df = 3; 47) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 "],["lab-ii-panel-data.html", "3 Lab II: Panel Data 3.1 Load the data from Oxford_Covid_data_US_latest.csv 3.2 Data Organization 3.3 Use the lag function in dplyr to create lagged variables for cases and deaths. Also create “difference” (e.g., dCases) that is the change in cases for each state by date. 3.4 Merge the above data frame to data in USstates.xlsx 3.5 Estimate a pooled model of total cases per capita as a function of state policy. Discuss. 3.6 Estimate a one-way fixed effect model where the fixed effect is state. (Note that state is indicated in a variable called RegionName.) Estimate using both LSDV and the de-meaned version in the plm package. Can you identify a source of bias? 3.7 Estimate a two-way fixed effect model where the fixed effects are state and date. Estimate using both LSDV and the de-meaned version in the plm package. Does this model address the source of bias identified earlier?", " 3 Lab II: Panel Data 3.0.1 Preparation # Load packages used in this session of R library(knitr) library(tidyverse) library(plm) library(readr) library(readxl) # Package to read Excel data opts_chunk$set(echo = TRUE) options(digits = 2) In this lab we will estimate standard panel data models on covid policy and cases/deaths in U.S. states. This is not a full-fledged analysis, but rather an initial exploration of the data that illustrates how fixed effects models work. 3.1 Load the data from Oxford_Covid_data_US_latest.csv Oxford provides data on covid deaths/cases and policy variables by day by state. For more background, see this data archive or this story that uses the data. We will use a variable called GovernmentResponseIndex. For details, see this. (And feel free to experiment with the other measures.) 3.2 Data Organization Load the Oxford_Covid_data_US_latest.csv data Limit it to U.S. states (CountryName == “United States) Create the following variables: RegionName, RegionCode, Date, GovernmentResponseIndex, ConfirmedCases and ConfirmedDeaths Add a variable to this data frame using the following code (this variable will help us when merging below) mutate(&quot;stAbbrev&quot; = str_replace_all(string = RegionCode, pattern = &quot;US_&quot;, replacement = &quot;&quot; )) Show the first three variables of the first three lines of this data frame. # Load and filter state stringency data stPolicy = read_csv(&quot;Data/Oxford_Covid_data_US_latest.csv&quot;) %&gt;% filter(CountryName == &quot;United States&quot; &amp; is.na(RegionName) == 0 &amp; RegionName != &quot;&quot; &amp; RegionCode != &quot;US_VI&quot;) %&gt;% dplyr::select(RegionName, RegionCode, Date, ContainmentHealthIndex, GovernmentResponseIndex, StringencyIndex, ConfirmedCases, ConfirmedDeaths) %&gt;% mutate(&quot;stAbbrev&quot; = str_replace_all(string = RegionCode, pattern = &quot;US_&quot;, replacement = &quot;&quot; )) ## Rows: 20724 Columns: 69 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (25): CountryName, CountryCode, RegionName, RegionCode, Jurisdiction, C1_Notes, C2_Notes, C3_Notes, C4_Notes, C5_N... ## dbl (43): Date, C1_School closing, C1_Flag, C2_Workplace closing, C2_Flag, C3_Cancel public events, C3_Flag, C4_Restri... ## lgl (1): M1_Wildcard ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Show the first three variables of first three lines stPolicy[1:3, 1:3] ## # A tibble: 3 × 3 ## RegionName RegionCode Date ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Alaska US_AK 20200101 ## 2 Alaska US_AK 20200102 ## 3 Alaska US_AK 20200103 3.3 Use the lag function in dplyr to create lagged variables for cases and deaths. Also create “difference” (e.g., dCases) that is the change in cases for each state by date. See slide “Creating differenced data in R” in Topic 3 class slides. ## Create lagged value stPolicy = stPolicy %&gt;% group_by(RegionName) %&gt;% mutate(lagCases = dplyr::lag(ConfirmedCases, order_by = Date), dCases = ConfirmedCases - lagCases, lagDeaths = dplyr::lag(ConfirmedDeaths, order_by = Date), dDeaths = ConfirmedDeaths - lagDeaths) %&gt;% ungroup() 3.4 Merge the above data frame to data in USstates.xlsx Merge by state abbreviation Create per capita measures of change in deaths and cases (e.g. “deathsPC” = 10000*dDeaths/pop2019). Check your data by looking at level and lagged data for a given state for a few years. The lagged data should match up to the previous period observation. # Load excel data stFacts &lt;- read_excel(&quot;Data/USstates.xlsx&quot;, sheet = &quot;data&quot;) # Merge with data frame and create per capita data dfState &lt;- stPolicy %&gt;% left_join(stFacts, by = c(&quot;stAbbrev&quot; = &quot;stateAbbr&quot;)) %&gt;% mutate(&quot;deathsPC&quot; = 10000*dDeaths/pop2019, &quot;casesPC&quot; = 10000*dCases/pop2019) # Check data dfState %&gt;% filter(RegionName == &quot;California&quot;) %&gt;% dplyr::select(RegionName, Date, ConfirmedDeaths, lagDeaths, dDeaths, deathsPC) %&gt;% slice(245:248) ## # A tibble: 4 × 6 ## RegionName Date ConfirmedDeaths lagDeaths dDeaths deathsPC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 California 20200901 13150 5044 8106 2.05 ## 2 California 20200902 13317 5065 8252 2.09 ## 3 California 20200903 13493 5130 8363 2.12 ## 4 California 20200904 13638 5171 8467 2.14 3.5 Estimate a pooled model of total cases per capita as a function of state policy. Discuss. ols.1 = lm(casesPC ~ GovernmentResponseIndex, data = dfState) summary(ols.1) ## ## Call: ## lm(formula = casesPC ~ GovernmentResponseIndex, data = dfState) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7972 20 215 342 1120 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -19.67 14.78 -1.33 0.18 ## GovernmentResponseIndex -5.04 0.31 -16.28 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 815 on 18547 degrees of freedom ## (1392 observations deleted due to missingness) ## Multiple R-squared: 0.0141, Adjusted R-squared: 0.014 ## F-statistic: 265 on 1 and 18547 DF, p-value: &lt;2e-16 3.6 Estimate a one-way fixed effect model where the fixed effect is state. (Note that state is indicated in a variable called RegionName.) Estimate using both LSDV and the de-meaned version in the plm package. Can you identify a source of bias? fe.1 = lm(casesPC ~ GovernmentResponseIndex + factor(RegionName), data = dfState) #summary(fe.1) coefficients(summary(fe.1))[1:2,] ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 404.5 33.55 12 2.4e-33 ## GovernmentResponseIndex -5.7 0.26 -22 7.5e-107 fe.1plm = plm(casesPC ~ GovernmentResponseIndex, data = dfState, index = c(&quot;RegionName&quot;,&quot;Date&quot;), model=&quot;within&quot;) summary(fe.1plm) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = casesPC ~ GovernmentResponseIndex, data = dfState, ## model = &quot;within&quot;, index = c(&quot;RegionName&quot;, &quot;Date&quot;)) ## ## Unbalanced Panel: n = 51, T = 349-369, N = 18549 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -6256.0 -95.8 31.4 152.4 1915.2 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## GovernmentResponseIndex -5.657 0.256 -22.1 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 7.17e+09 ## Residual Sum of Squares: 6.98e+09 ## R-Squared: 0.0257 ## Adj. R-Squared: 0.023 ## F-statistic: 488.43 on 1 and 18497 DF, p-value: &lt;2e-16 3.7 Estimate a two-way fixed effect model where the fixed effects are state and date. Estimate using both LSDV and the de-meaned version in the plm package. Does this model address the source of bias identified earlier? fe.2 = lm(casesPC ~ GovernmentResponseIndex + factor(RegionName) + factor(Date), data = dfState) #summary(fe.2) # Show non-fixed effect coefficients coefficients(summary(fe.2))[1:2,] ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 393.5 86.62 4.5 5.6e-06 ## GovernmentResponseIndex -6.8 0.76 -9.0 2.4e-19 fe.2plm = plm(casesPC ~ GovernmentResponseIndex, data = dfState, index = c(&quot;RegionName&quot;,&quot;Date&quot;), model=&quot;within&quot;, effect=&quot;twoways&quot;) summary(fe.2plm) ## Twoways effects Within Model ## ## Call: ## plm(formula = casesPC ~ GovernmentResponseIndex, data = dfState, ## effect = &quot;twoways&quot;, model = &quot;within&quot;, index = c(&quot;RegionName&quot;, ## &quot;Date&quot;)) ## ## Unbalanced Panel: n = 51, T = 349-369, N = 18549 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -5510 -239 -14 173 1736 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## GovernmentResponseIndex -6.822 0.758 -9 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 6e+09 ## Residual Sum of Squares: 5.97e+09 ## R-Squared: 0.00445 ## Adj. R-Squared: -0.0186 ## F-statistic: 81.0794 on 1 and 18129 DF, p-value: &lt;2e-16 3.7.1 This is an initial analysis. We would also want to think through whether it is useful to control for days of the week (there is a well-known pattern in reporting across days of the week) assess the data for outliers (e.g, min(dfState$casesPC, na.rm = TRUE)) consider a lagged dependent variable For more, read this "],["lab-iii-2sls-instrumental-variables.html", "4 Lab III: 2SLS &amp; Instrumental Variables 4.1 Estimate a basic OLS model with “Do others regard you as religious” as the dependent variable as a function of Hajj2006. Explain how there might be endogeneity. 4.2 Explain how the “success” variable may satisfy the conditions for a instrumental variable. 4.3 Estimate a 2SLS model Religious as a function of Hajj2006. 4.4 Show the first stage from the 2SLS model above. Explain the implications of the results. 4.5 Add covariates for age, literacy, urban, group size and gender to the 2SLS model Religious as a function of Hajj2006. What is different? Which variables are included in the first stage? 4.6 Run multiple 2SLS models with OssamaIncorrect, GovtForce, NatlInterest, Happy, GirlsSchool and JobsWomen variables as dependent variables. Use the list of covariates from earlier. If you want, try using a loop or lapply (but not necessary).", " 4 Lab III: 2SLS &amp; Instrumental Variables 4.0.1 Preparation ## Packages library(haven) ## Package to read Stata data library(ivreg) ## Package to run 2sls library(fixest) ## This package can also run 2SLS library(tidyverse) ## For tidyverse commands library(here) ## Importing Data ## Loading Data hajj_public &lt;- read_dta(&quot;Data/hajj_public.dta&quot;) Do important life experiences influence political and social views? In particular, does performing the Hajj pilgrimage to Mecca affect the views of pilgrims? David Clingingsmith, Asim Ijaz Khwaja, and Michael Kremer (2009) analyze this question by using two-stage least squares to compare successful and unsuccessful applicants in a lottery used by Pakistan to allocate Hajj visas. We will conduct pared-down models. The paper creates indices and implements additional statistical procedures to produce a broader and clearer picture. It is not a bad idea to read this paper to see how we can extend the methods we learn in class to your own work. I posted the paper on Canvas for your convenience. Data description Variable Description hajj2006 Went on Hajj trip in 2006 success Won the lottery to have expenses covered for Hajj ptygrp Categorical variable indicating size of party for Hajj trip smallpty 1 if small party group, 0 otherwise urban 1 if live in urban area, 0 otherwise age Age female 1 if female, 0 otherwise literate 1 if literate, 0 otherwise x_s7q10 Natl affairs: How often do you follow national affairs in the news on television or on the radio? Binary: 0=Twice a week or less, 1=Several times a week or more x_s14aq10 Religious: Do others regard you as religious? Binary: 1=Religious, 0=Not Religious x_s10bq4 OssamaIncorrect: Do you believe goals Ossama is fighting for are correct? Binary: 1=Not Correct at All/Slightly Incorrect, 0=Correct/Absolutely Correct x_s7q12a GovtForce: Govt should force people to conform to Islamic injunctions. Binary: 1=Agree Strongly/Agree, 0=Neutral/Disagree/Strongly Disagree x_s7q1 NatlInterest How interested would you say you are in national affairs? Binary: 0=Not interested, 1=Interested x_s3q3 Happy: how happy are you? From 1 (not at all happy) to 4 (very happy). x_s10eq2 GirlsSchool: In your opinion, girls should attend school. Binary: 0=Disagree, 1=Agree s10dq1 JobsWomen: When jobs are scarce, men should always have more right to a job than women. Binary: 0=Generally agree, 1=Generally Disagree More details on these and other variables are available in Appendix 3 of the paper. If you cannot access the version, the SSRN version works as well. 4.1 Estimate a basic OLS model with “Do others regard you as religious” as the dependent variable as a function of Hajj2006. Explain how there might be endogeneity. hajj_public %&gt;% lm(x_s14aq10~hajj2006, data=.) %&gt;% broom::tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.767 0.0154 49.8 4.94e-323 ## 2 hajj2006 0.0851 0.0199 4.27 2.09e- 5 There may be endogeneity due to baseline bias caused by the religiosity of respondents. A more religious respondent may be more likely to go on a Hajj trip and be classified as religious by others. Going to church, like actually being religious, is also a factor that may be correlated with x and lurking in the error term. 4.2 Explain how the “success” variable may satisfy the conditions for a instrumental variable. The two conditions, inclusion and exclusion, are: \\[Cov(X,Z)\\ne0\\] &amp; \\[Cov(Z,\\epsilon)=0\\] The lottery is randomizes, which means it is not correlated with the error term, or anything else other than the treatment variable, in our model. Further, it meaningfully effects our key independent which is tested below. 4.3 Estimate a 2SLS model Religious as a function of Hajj2006. ## With library(ivreg) hajj_public %&gt;% ivreg(x_s14aq10~hajj2006 | success, data=.) %&gt;% broom::tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.757 0.0169 44.9 1.09e-281 ## 2 hajj2006 0.101 0.0231 4.40 1.18e- 5 4.4 Show the first stage from the 2SLS model above. Explain the implications of the results. hajj_public %&gt;% lm(hajj2006~success, data=.) %&gt;% broom::tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.137 0.00893 15.4 6.35e-50 ## 2 success 0.854 0.0122 69.9 0 The t-score is 69.87 which is much higher than the 3 threshold. Our instrument meets in the inclusion condition. 4.5 Add covariates for age, literacy, urban, group size and gender to the 2SLS model Religious as a function of Hajj2006. What is different? Which variables are included in the first stage? hajj_public %&gt;% ivreg(x_s14aq10~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data=.) ## ## Call: ## ivreg(formula = x_s14aq10 ~ hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data = .) ## ## Coefficients: ## (Intercept) hajj2006 age literate ptygrp female urban ## 0.493486 0.101217 0.002687 0.014634 -0.000955 0.131693 0.067398 4.6 Run multiple 2SLS models with OssamaIncorrect, GovtForce, NatlInterest, Happy, GirlsSchool and JobsWomen variables as dependent variables. Use the list of covariates from earlier. If you want, try using a loop or lapply (but not necessary). ## OssamaIncorrect hajj_public %&gt;% ivreg(x_s10bq4~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data=.) %&gt;% broom::tidy() ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.158 0.0682 2.32 0.0207 ## 2 hajj2006 0.0576 0.0247 2.33 0.0198 ## 3 age -0.00160 0.000844 -1.89 0.0585 ## 4 literate -0.00122 0.0283 -0.0430 0.966 ## 5 ptygrp -0.0151 0.00729 -2.08 0.0381 ## 6 female 0.0486 0.0240 2.03 0.0431 ## 7 urban 0.00542 0.0252 0.215 0.830 ## GovtForce hajj_public %&gt;% ivreg(x_s7q12a~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data=.) %&gt;% broom::tidy() ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.941 0.0487 19.3 2.04e-74 ## 2 hajj2006 -0.0308 0.0176 -1.75 8.08e- 2 ## 3 age 0.000518 0.000626 0.827 4.08e- 1 ## 4 literate -0.0255 0.0186 -1.37 1.70e- 1 ## 5 ptygrp -0.00846 0.00499 -1.69 9.05e- 2 ## 6 female -0.0228 0.0170 -1.34 1.80e- 1 ## 7 urban 0.000708 0.0168 0.0423 9.66e- 1 ## NatlInterest hajj_public %&gt;% ivreg(x_s7q1~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data=.) %&gt;% broom::tidy() ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.304 0.0783 3.88 1.11e- 4 ## 2 hajj2006 0.0155 0.0283 0.545 5.86e- 1 ## 3 age -0.000378 0.000999 -0.379 7.05e- 1 ## 4 literate 0.212 0.0300 7.05 2.80e-12 ## 5 ptygrp -0.00736 0.00802 -0.918 3.59e- 1 ## 6 female -0.117 0.0273 -4.30 1.84e- 5 ## 7 urban 0.0897 0.0267 3.36 8.01e- 4 ## Happy hajj_public %&gt;% ivreg(x_s3q3~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data=.) %&gt;% broom::tidy() ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.51 0.110 32.0 6.34e-174 ## 2 hajj2006 -0.0641 0.0396 -1.62 1.06e- 1 ## 3 age -0.00344 0.00140 -2.45 1.44e- 2 ## 4 literate 0.145 0.0420 3.45 5.81e- 4 ## 5 ptygrp 0.00463 0.0112 0.415 6.78e- 1 ## 6 female -0.0673 0.0383 -1.76 7.89e- 2 ## 7 urban 0.0169 0.0372 0.454 6.50e- 1 ## Girl School hajj_public %&gt;% ivreg(x_s10eq2~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female, data=.) %&gt;% broom::tidy() ## Warning in ivreg.fit(X, Y, Z, weights, offset, method, ...): more regressors than instruments ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.894 0.0355 25.2 4.07e-118 ## 2 hajj2006 0.0271 0.0133 2.04 4.16e- 2 ## 3 age 0.000258 0.000469 0.550 5.83e- 1 ## 4 literate 0.0414 0.0140 2.96 3.10e- 3 ## 5 ptygrp -0.00283 0.00373 -0.758 4.49e- 1 ## 6 female 0.00569 0.0128 0.443 6.58e- 1 ## 7 urban NA NA NA NA ## Jobs Women hajj_public %&gt;% ivreg(x_s10dq1~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female, data=.) %&gt;% broom::tidy() ## Warning in ivreg.fit(X, Y, Z, weights, offset, method, ...): more regressors than instruments ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.153 0.0493 3.10 0.00194 ## 2 hajj2006 -0.00687 0.0185 -0.371 0.711 ## 3 age -0.00104 0.000651 -1.59 0.112 ## 4 literate 0.0241 0.0194 1.24 0.215 ## 5 ptygrp -0.00932 0.00519 -1.80 0.0724 ## 6 female 0.0569 0.0179 3.18 0.00148 ## 7 urban NA NA NA NA ## Loop ## DVs dvs &lt;- c(&quot;hajj_public$x_s10bq4&quot;, &quot;hajj_public$x_s7q12a&quot;, &quot;hajj_public$x_s7q1&quot;, &quot;hajj_public$x_s3q3&quot;, &quot;hajj_public$x_s10eq2&quot;, &quot;hajj_public$x_s10dq1&quot;) ## Loop for(i in 1:length(dvs)){ model &lt;- paste(&quot;model&quot;,i, sep=&quot;&quot;) m &lt;- ivreg(as.formula(paste(dvs[i],&quot;~ hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban&quot;)), data = hajj_public) assign(model,m)} model_list &lt;- list(model1, model2, model3, model4, model5, model6) b &lt;- round(sapply(model_list, function(x) x$coefficients[&quot;hajj2006&quot;]) , 2) t &lt;- round(sapply(model_list, function(x) { summary(x)$coefficients[&quot;hajj2006&quot;, 3]}) , 2) names &lt;- c(&quot;OsamaIncorrect&quot;, &quot;GovtForce&quot;, &quot;NatlInterest&quot;, &quot;Happy&quot;, &quot;GirlsSchool&quot;, &quot;JobsWomen&quot;) data.frame(DV = names, b, t) %&gt;% arrange(desc(b)) ## DV b t ## 1 OsamaIncorrect 0.06 2.33 ## 2 GirlsSchool 0.03 1.99 ## 3 NatlInterest 0.02 0.55 ## 4 JobsWomen -0.01 -0.44 ## 5 GovtForce -0.03 -1.75 ## 6 Happy -0.06 -1.62 "],["lab-iv-data-visualization.html", "5 Lab IV: Data Visualization 5.1 Base R Version 5.2 Base R Version 2 5.3 library(ggplot) Version 1 5.4 library(ggplot) Version 2 5.5 Alternative Plot of Your Choice", " 5 Lab IV: Data Visualization 5.0.1 Preparation # Load packages used in this session of R library(knitr) library(tidyverse) library(ggplot2) opts_chunk$set(echo = TRUE) options(digits = 2) Our goal is to visualize the difference between the population percent (popPct) and the survey percent (svyPct) for various age groups. We’ll use the data in the table below (and of course, a full viz would include more subgroups). Create designs on how to present this information. Be ready to share concept and actual viz with the entire class. You can either work individually or in small groups. Do not include code with your visualizations. Instead, create an appendix that displays each code chunk at the end of the document. Make sure there are no warnings or messages displaying too. Use the simulated data to make at least two plots: one in Base R and one in library(ggplot). Then you can use a dataset of your choice for the last two visualizations or keep working with the fake data. df &lt;- data.frame(&quot;age&quot; = c(&quot;18 to 29&quot;, &quot;36 to 50&quot;, &quot;51 to 64&quot;, &quot;65+&quot;), &quot;popPct&quot; = c(29, 21, 30, 20), &quot;svyPct&quot; = c(19, 21, 32, 28)) kable(df, caption = &quot;Table: Population and survey percentages by age group&quot;) Table 5.1: Table: Population and survey percentages by age group age popPct svyPct 18 to 29 29 19 36 to 50 21 21 51 to 64 30 32 65+ 20 28 5.1 Base R Version Age18to29 &lt;- c(19, 29) Age36to50 &lt;- c(21,21) Age51to64 &lt;- c(32, 30) Over65 &lt;- c(28, 20) age_groups &lt;- cbind(Age18to29, Age36to50, Age51to64, Over65) barplot(age_groups, beside=T, xlab=&quot;Age Group&quot;, names.arg= c(&quot;18 - 29&quot;, &quot;36 - 50&quot;, &quot;51 - 64&quot;, &quot;65+&quot;), ylab=&quot;Percent&quot;, main = &quot;Percent Surveyed and Percent in Population by Age Group&quot;, ylim = c(0,35), las=1) legend(&quot;bottomleft&quot;,c(&quot;Surveyed %&quot;, &quot;Population %&quot;), fill=c(&quot;black&quot;, &quot;light gray&quot;), horiz=FALSE, cex=0.73, bg=&quot;white&quot;) 5.2 Base R Version 2 dataDiff = data.frame(category = c(&quot;18 to 35&quot;, &quot;36 to 50&quot;, &quot;51 to 64&quot;, &quot;65+&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Black&quot;, &quot;No HS&quot;, &quot;HS grad&quot;, &quot;Some college&quot;, &quot;2 year college&quot;, &quot;4 year college&quot;, &quot;Post-grad&quot;)) dataDiff$diff = -1*c(28.8 - 19, 21.1-20.9, 29.8-31.8, 20.1 - 28.4, 50.8 - 47.3, 49.2 - 52.7, 11.8 - 8.9, 6.8 - 2.8, 30.6 - 19.7, 23 - 15.7, 10.6 - 11.3, 18.7 - 28.6, 10.4 - 21.9) par(mfrow=c(1, 1), # mar(south, west, north, east) mar=c(2., 0.25, 0.5, 0.5), oma=c(1., 0.25, 0.5, 0.3)) plot(c(-16, 12), c(0.5, dim(dataDiff)[1]+1), type=&quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, bty = &#39;n&#39;) axis(1, at = seq(-12, 12, by = 2), labels = seq(-12, 12, by = 2), tick = T, cex.axis = 1.0, mgp = c(2,.4,0)) mtext(&quot;Degree of misrepresentation&quot;, side = 1, line = 1.75, at = 0, cex = 1) abline(v = 0, col = &quot;grey&quot;) text(9, dim(dataF)[1]+1, &quot;Over-represented&quot;, cex = 0.9, col = &quot;darkgrey&quot;) text(-10, dim(dataF)[1]+1, &quot;Under-represented&quot;, cex = 0.9, col = &quot;darkgrey&quot;) # Plot age group points(dataDiff$diff[1:4], dim(dataDiff)[1]:(dim(dataDiff)[1]-3), col = &quot;darkblue&quot;, pch = 20, cex = 1.5) lines(dataDiff$diff[1:4], dim(dataDiff)[1]:(dim(dataDiff)[1]-3), col = &quot;darkblue&quot;, lwd = 3) for(aa in 1:4){ text(-14., dim(dataDiff)[1]-aa+1, dataDiff$category[aa], cex = 0.9, col = &quot;darkblue&quot;) } abline(h = dim(dataDiff)[1]-aa+0.5, col = &quot;grey&quot;) # Plot gender points(dataDiff$diff[5:6], (dim(dataDiff)[1]-4):(dim(dataDiff)[1]-5), col = &quot;#cc6600&quot;, pch = 20, cex = 1.5) lines(dataDiff$diff[5:6], (dim(dataDiff)[1]-4):(dim(dataDiff)[1]-5), col = &quot;#cc6600&quot;, lwd = 3) for(aa in 5:6){ text(-14., dim(dataDiff)[1]-aa+1, dataDiff$category[aa], cex = 0.9, col = &quot;#cc6600&quot;) } abline(h = dim(dataDiff)[1]-aa+0.5, col = &quot;grey&quot;) # Plot race points(dataDiff$diff[7], (dim(dataDiff)[1]-6):(dim(dataDiff)[1]-6), col = &quot;darkred&quot;, pch = 20, cex = 1.5) lines(dataDiff$diff[7], (dim(dataDiff)[1]-6):(dim(dataDiff)[1]-6), col = &quot;darkred&quot;, lwd = 3) for(aa in 7){ text(-14., dim(dataDiff)[1]-aa+1, dataDiff$category[aa], cex = 0.9, col = &quot;darkred&quot;) } abline(h = dim(dataDiff)[1]-aa+0.5, col = &quot;grey&quot;) # Plot education points(dataDiff$diff[8:13], (dim(dataDiff)[1]-7):(dim(dataDiff)[1]-12), col = &quot;darkgreen&quot;, pch = 20, cex = 1.5) lines(dataDiff$diff[8:13], (dim(dataDiff)[1]-7):(dim(dataDiff)[1]-12), col = &quot;darkgreen&quot;, lwd = 3) for(aa in 8:13){ text(-14., dim(dataDiff)[1]-aa+1, dataDiff$category[aa], cex = 0.9, col = &quot;darkgreen&quot;) } 5.3 library(ggplot) Version 1 df %&gt;% mutate(Population = popPct, Survey = svyPct) %&gt;% dplyr::select(-popPct, -svyPct) %&gt;% pivot_longer(-age, names_to=&quot;Group&quot;, values_to=&quot;Percent&quot;) %&gt;% ggplot(aes(x=age, y=Percent, fill=Group)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + scale_fill_grey() + theme_bw() + labs(x = &quot;Age Group&quot;, y = &quot;Percent&quot;, title = &quot;Population and Survey Sample Proportions by Age Group&quot;) 5.4 library(ggplot) Version 2 df %&gt;% mutate(Population = popPct, Survey = svyPct) %&gt;% dplyr::select(-popPct, -svyPct) %&gt;% pivot_longer(-age, names_to=&quot;Group&quot;, values_to=&quot;Percent&quot;) %&gt;% ggplot(aes(x=age, y=Percent, fill=Group)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + coord_flip() + scale_fill_grey() + theme_minimal() + labs(x = &quot;Age Group&quot;, y = &quot;Percent&quot;, title = &quot;Population and Survey Sample Proportions by Age Group&quot;) 5.5 Alternative Plot of Your Choice library(apyramid) df %&gt;% mutate(Population = popPct, Survey = svyPct) %&gt;% dplyr::select(-popPct, -svyPct) %&gt;% pivot_longer(-age, names_to=&quot;Group&quot;, values_to=&quot;Percent&quot;) %&gt;% mutate(age = as.factor(age)) %&gt;% age_pyramid(data = ., age_group = &quot;age&quot;, split_by = &quot;Group&quot;, count = &quot;Percent&quot;, show_midpoint = FALSE) + scale_fill_grey() + theme_bw() + labs(x=&quot;Age Group&quot;, y=&quot;Percent&quot;, fill=NULL, title = &quot;Percent Surveyed and Percent in Population by Age Group&quot;) "],["lab-v-experiments.html", "6 Lab V: Experiments 6.1 Why is an experiment useful in this context? 6.2 Is there evidence of balance in the treatment? Why is this relevant? Check for balance of each treatment with respect to age category, gender, education, income, voting for Sisi and not voting for non-boycott reasons. 6.3 Is there evidence of an overall unbalanced sample? Look at gender and discuss results in light of your answer on treatment-balance above. 6.4 What is the effect of the treatments on whether respondents viewed the police tactics as justified? Run one model with only the treatment variables and another that includes controls for age category, gender, education, income, voting for Sisi and not voting for non-boycott reasons. 6.5 What is the effect of the treatments on whether respondents viewed the police tactics as justified? 6.6 Test whether the effects of treatment 1 (police-favorable message) are the same as treatment 3 (both sides) for each dependent variable. Feel free to use the linearHypothesis function in the car package. 6.7 In some political contexts, messages have a different effect on people with less engagement in politics. Are there heterogeneous treatment effects by those who did not vote for reasons other than to boycott the election and those who did not? Feel free to explore multiple other ways there could be heterogeneous effects (e.g., perhaps there are different effects on Sisi supporters), but you only need to report on possible differential effects of the treatments on the “not-accountable” dependent variable.", " 6 Lab V: Experiments 6.0.1 Preparation library(knitr) library(haven) library(tidyverse) library(car) Williamson and Malik (2020) used a survey experiment to examine the public response to how Egyptians responded to different framings regarding a protest in which protesters were killed by police. All respondents were given the following statement: “Last year, police raided an apartment in 6 October City. During the raid, they killed 9 members of the Muslim Brotherhood.” Respondents were assigned with equal probability to a control group that got no more information or one of three treatment groups that received additional information. Respondents in the first treatment group were provided a paragraph that justified the killings from the security forces perspective. Respondents in the second treatment group were provided a paragraph that criticized the killings from a human rights perspective. Respondents in the third treatment group were provided both the security forces and human rights perspectives. Two outcomes were measured: if respondents thought the police tactics were justified, and if the police should the police be held accountable for killing the nine men. So that both variables are coded in same direction, this variable is coded as 1 if the respondent gave a pro-police answer (saying the police did not need to be held accountable). Data description Variable Description just_binary_main 1 if the respondent said the police tactics were justified; 0 otherwise. not_acc_binary_main 1 if the respondent said the police should definitely or probably not be held accountable; 0 otherwise. We call this variable no-accountability treat_b1 1 if respondent was given the pro-police paragraph treat_b2 1 if respondent was given the human rights paragraph treat_b3 1 if respondent was given both the pro-police and human rights paragraphs sisi_vote 1 if the respondent voted for Sisi not_voteboycott 1 if the respondent did not vote for some other reason than boycotting the election opposition_vote_boycott 1 if the respondent either voted for the opposition candidate or boycotted the election education from 1 (no formal education) to 8 (graduate degree). Values from 6 to 8 went to college. income 1 = Less than 500 pounds per month, 2 = 500 to 1000 pounds per month, 3 = 1000 to 4000 pounds per month, 4 = 4000 to 10000 pounds per month, 5 = More than 10000 pounds per month male 1 for men, 0 otherwise age_cat age categories: 1 is less than 35, 2 is 35 to 50 and 3 is over 50. Published paper (paywalled) Draft version of the paper 6.1 Why is an experiment useful in this context? Simply asking respondents if they agree or disagree with the police does not answer if and how different messages work. If respondents were simply asked if they agree with the either the pro-police or human rights messages it may be difficult to separate out whether the message was persuasive or whether the respondent was predisposed to approve of the argument. 6.2 Is there evidence of balance in the treatment? Why is this relevant? Check for balance of each treatment with respect to age category, gender, education, income, voting for Sisi and not voting for non-boycott reasons. ## Loading saved dataset load(&quot;~/GOVT8002/Spring 2024/Lab V/Ch10_lab_EgyptProtests.RData&quot;) ## Removing NAs dta &lt;- dta[is.na(dta$just_binary_main)==0 &amp; is.na(dta$not_acc_binary_main)==0,] dep_vars &lt;- dta %&gt;% dplyr::select(age_cat, male, education, sisi_vote, not_voteboycott, income) b_test &lt;- lapply(dep_vars, function(bal) { lm(bal ~ dta$treat_b1 + dta$treat_b2 + dta$treat_b3)}) ## Extracting Slopes and P Values slopes &lt;- round(sapply(b_test, function(x) x$coefficients), 2) p &lt;- round(sapply(b_test, function(x) { summary(x)$coefficients[,4]}), 2) ## Showing Results balance_results = data.frame(b1= slopes[2,], b1.p = p[2,], b2= slopes[3,], b2.p = p[3,], b3= slopes[4,], b3.p = p[4,]) balance_results ## b1 b1.p b2 b2.p b3 b3.p ## age_cat 0.05 0.60 0.01 0.89 -0.01 0.89 ## male 0.01 0.77 0.00 0.90 0.05 0.18 ## education 0.45 0.01 0.27 0.12 0.04 0.82 ## sisi_vote 0.02 0.74 0.05 0.36 0.09 0.12 ## not_voteboycott -0.02 0.71 -0.02 0.76 -0.05 0.37 ## income 0.18 0.10 0.06 0.55 0.24 0.03 There is some evidence of imbalance. 6.3 Is there evidence of an overall unbalanced sample? Look at gender and discuss results in light of your answer on treatment-balance above. table(dta$gender) ## ## 1 2 ## 497 76 dta %&gt;% group_by(gender) %&gt;% count() ## start_date end_date consent age younger18 gender education degree news pres_election gov_eval cont_1 cont_2 ## 1 9/5/2016 23:29 9/5/2016 23:37 1 6 NA 1 8 4 2 4 1 NA 1 ## 2 9/5/2016 23:33 9/5/2016 23:38 1 21 NA 1 7 7 1 3 5 NA NA ## 3 9/5/2016 23:46 9/5/2016 23:53 1 19 NA 1 5 NA 2 1 3 NA NA ## 4 9/6/2016 2:31 9/6/2016 2:35 1 3 NA 1 8 NA 1 3 5 1 NA ## 5 9/6/2016 2:26 9/6/2016 2:35 1 22 NA 1 5 NA 3 4 3 NA NA ## 6 9/6/2016 2:46 9/6/2016 2:50 1 5 NA 1 7 NA 1 1 4 NA NA ## 7 9/5/2016 23:02 9/5/2016 23:08 1 23 NA 1 5 NA 1 1 2 NA NA ## 8 9/6/2016 3:27 9/6/2016 3:32 1 20 NA 1 8 6 1 1 5 NA NA ## 9 9/6/2016 3:28 9/6/2016 3:36 1 23 NA 1 5 NA 2 1 3 NA 1 ## 10 9/6/2016 3:26 9/6/2016 3:36 1 2 NA 1 5 NA 3 4 3 NA NA ## 11 9/6/2016 3:50 9/6/2016 3:53 1 23 NA 1 8 NA 1 1 1 NA NA ## 12 9/6/2016 5:12 9/6/2016 5:17 1 22 NA 1 7 11 1 4 5 1 NA ## 13 9/6/2016 5:19 9/6/2016 5:25 1 21 NA 1 7 11 1 3 5 NA NA ## 14 9/6/2016 5:23 9/6/2016 5:33 1 23 NA 1 3 NA 2 2 4 NA NA ## 15 9/6/2016 5:52 9/6/2016 5:59 1 22 NA 2 5 NA 2 1 2 NA NA ## 16 9/6/2016 6:43 9/6/2016 7:00 1 20 NA 1 5 NA 1 1 5 NA NA ## cont_3 treat_11 treat_12 treat_13 treat_21 treat_22 treat_23 treat_31 treat_32 treat_33 justified_1 account_1 cont_b1 ## 1 NA NA NA NA NA NA NA NA NA NA 3 3 0 ## 2 NA NA NA NA NA 1 NA NA NA NA 5 1 0 ## 3 NA NA NA 1 NA NA NA NA NA NA 3 3 0 ## 4 NA NA NA NA NA NA NA NA NA NA 5 1 0 ## 5 NA NA NA NA NA NA NA 1 NA NA 3 1 0 ## 6 1 NA NA NA NA NA NA NA NA NA 5 1 0 ## 7 1 NA NA NA NA NA NA NA NA NA 1 5 0 ## 8 NA 1 NA NA NA NA NA NA NA NA 1 1 0 ## 9 NA NA NA NA NA NA NA NA NA NA 2 3 1 ## 10 NA NA NA NA NA NA 1 NA NA NA 4 2 0 ## 11 NA NA NA NA NA NA 1 NA NA NA 1 5 0 ## 12 NA NA NA NA NA NA NA NA NA NA 5 1 0 ## 13 NA NA 1 NA NA NA NA NA NA NA 5 1 1 ## 14 NA 1 NA NA NA NA NA NA NA NA 5 1 0 ## 15 1 NA NA NA NA NA NA NA NA NA 1 5 0 ## 16 NA NA NA NA NA NA 1 NA NA NA 4 1 0 ## treat_b1 treat_b2 treat_b3 justified_2 account_scale mb_incite prev_exp income gov priority sisi_term dem debrief ## 1 1 0 0 3 3 3 3 2 18 2 3 3 1 ## 2 0 1 0 5 1 5 1 2 20 1 1 5 1 ## 3 0 0 1 3 3 3 2 3 9 2 2 3 1 ## 4 1 0 0 3 1 3 1 2 20 1 4 5 1 ## 5 0 1 0 5 1 2 2 2 20 2 4 5 1 ## 6 0 1 0 5 1 3 1 3 16 1 3 5 1 ## 7 1 0 0 1 5 1 1 3 18 2 3 4 NA ## 8 0 0 1 3 3 1 1 2 1 2 1 3 1 ## 9 0 0 0 3 1 3 2 3 11 2 2 4 1 ## 10 1 0 0 2 2 3 2 2 13 1 3 4 1 ## 11 0 0 1 1 5 1 1 3 15 2 4 1 1 ## 12 1 0 0 4 1 3 2 4 20 2 1 5 1 ## 13 0 0 0 5 1 4 1 3 14 1 1 4 1 ## 14 0 1 0 2 1 1 3 1 20 1 1 5 1 ## 15 0 0 1 1 5 1 2 2 11 2 4 1 1 ## 16 0 0 1 3 1 1 1 2 6 2 2 4 1 ## survey_num ad_num treatment_exper_main city male age_category just_binary_main just_binary_main_b not_acc_binary_main ## 1 1 1 1 0 1 2 0 1 0 ## 2 1 1 2 1 1 6 0 0 0 ## 3 1 1 3 0 1 4 0 1 0 ## 4 1 1 1 1 1 1 0 1 0 ## 5 1 1 2 1 1 7 0 0 0 ## 6 1 1 2 0 1 1 0 0 0 ## 7 1 1 1 0 1 8 1 1 1 ## 8 1 1 3 1 1 5 0 1 0 ## 9 1 1 0 1 1 8 0 1 0 ## 10 1 1 1 0 1 1 1 1 0 ## 11 1 1 3 0 1 8 1 1 1 ## 12 1 1 1 1 1 7 0 0 0 ## 13 1 1 0 0 1 6 0 0 0 ## 14 1 1 2 1 1 8 1 1 0 ## 15 1 1 3 1 0 7 1 1 1 ## 16 1 1 3 0 1 5 0 1 0 ## not_acc_binary_main_b acc_binary_main acc_binary_main_b justified_scale heard college sisi_vote not_voteboycott ## 1 1 0 1 3 0 TRUE FALSE TRUE ## 2 0 1 1 1 1 TRUE FALSE FALSE ## 3 1 0 1 3 0 FALSE TRUE FALSE ## 4 0 1 1 3 1 TRUE FALSE FALSE ## 5 0 1 1 1 0 FALSE FALSE TRUE ## 6 0 1 1 1 1 TRUE TRUE FALSE ## 7 1 0 0 5 1 FALSE TRUE FALSE ## 8 1 0 1 3 1 TRUE TRUE FALSE ## 9 0 1 1 3 0 FALSE TRUE FALSE ## 10 0 1 1 4 0 FALSE FALSE TRUE ## 11 1 0 0 5 1 TRUE TRUE FALSE ## 12 0 1 1 2 0 TRUE FALSE TRUE ## 13 0 1 1 1 1 TRUE FALSE FALSE ## 14 0 1 1 4 0 FALSE FALSE FALSE ## 15 1 0 0 5 0 FALSE TRUE FALSE ## 16 0 1 1 3 1 FALSE TRUE FALSE ## opposition_vote_boycott age_cat freq ## 1 FALSE 1 1 ## 2 TRUE 2 1 ## 3 FALSE 1 1 ## 4 TRUE 1 1 ## 5 FALSE 2 1 ## 6 FALSE 1 1 ## 7 FALSE 3 1 ## 8 FALSE 2 1 ## 9 FALSE 3 1 ## 10 FALSE 1 1 ## 11 FALSE 3 1 ## 12 FALSE 2 1 ## 13 TRUE 2 1 ## 14 TRUE 3 1 ## 15 FALSE 2 1 ## 16 FALSE 2 1 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 567 rows ] 6.4 What is the effect of the treatments on whether respondents viewed the police tactics as justified? Run one model with only the treatment variables and another that includes controls for age category, gender, education, income, voting for Sisi and not voting for non-boycott reasons. ## Without Controls dta %&gt;% lm(just_binary_main~treat_b1 + treat_b2 + treat_b3, data=.) %&gt;% summary() ## ## Call: ## lm(formula = just_binary_main ~ treat_b1 + treat_b2 + treat_b3, ## data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.459 -0.345 -0.315 0.541 0.685 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3154 0.0392 8.04 5e-15 *** ## treat_b1 0.1435 0.0558 2.57 0.01 * ## treat_b2 0.0134 0.0555 0.24 0.81 ## treat_b3 0.0299 0.0565 0.53 0.60 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.48 on 579 degrees of freedom ## Multiple R-squared: 0.0141, Adjusted R-squared: 0.00898 ## F-statistic: 2.76 on 3 and 579 DF, p-value: 0.0417 ## With Controls dta %&gt;% lm(just_binary_main~treat_b1 + treat_b2 + treat_b3 + factor(age_cat) + male + education + income + sisi_vote + not_voteboycott, data=.) %&gt;% summary() ## ## Call: ## lm(formula = just_binary_main ~ treat_b1 + treat_b2 + treat_b3 + ## factor(age_cat) + male + education + income + sisi_vote + ## not_voteboycott, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.790 -0.199 -0.101 0.333 0.945 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.077108 0.101560 0.76 0.448 ## treat_b1 0.118974 0.049977 2.38 0.018 * ## treat_b2 -0.024984 0.049672 -0.50 0.615 ## treat_b3 -0.016967 0.050694 -0.33 0.738 ## factor(age_cat)2 -0.036249 0.042545 -0.85 0.395 ## factor(age_cat)3 0.000590 0.047200 0.01 0.990 ## male 0.049475 0.053435 0.93 0.355 ## education -0.000762 0.012569 -0.06 0.952 ## income -0.001007 0.020909 -0.05 0.962 ## sisi_voteTRUE 0.549402 0.041442 13.26 &lt;2e-16 *** ## not_voteboycottTRUE 0.080694 0.045381 1.78 0.076 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4 on 516 degrees of freedom ## (56 observations deleted due to missingness) ## Multiple R-squared: 0.293, Adjusted R-squared: 0.279 ## F-statistic: 21.3 on 10 and 516 DF, p-value: &lt;2e-16 6.5 What is the effect of the treatments on whether respondents viewed the police tactics as justified? dta %&gt;% lm(not_acc_binary_main ~ treat_b1 + treat_b2 + treat_b3, data=.) %&gt;% summary() ## ## Call: ## lm(formula = not_acc_binary_main ~ treat_b1 + treat_b2 + treat_b3, ## data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.302 -0.294 -0.222 0.698 0.805 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.1946 0.0355 5.48 6.2e-08 *** ## treat_b1 0.0999 0.0505 1.98 0.048 * ## treat_b2 0.0268 0.0502 0.53 0.593 ## treat_b3 0.1075 0.0511 2.10 0.036 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.43 on 579 degrees of freedom ## Multiple R-squared: 0.0113, Adjusted R-squared: 0.00618 ## F-statistic: 2.21 on 3 and 579 DF, p-value: 0.0862 dta %&gt;% lm(not_acc_binary_main ~ treat_b1 + treat_b2 + treat_b3+ factor(age_cat) + male + education + income + sisi_vote + not_voteboycott, data=.) %&gt;% summary() ## ## Call: ## lm(formula = not_acc_binary_main ~ treat_b1 + treat_b2 + treat_b3 + ## factor(age_cat) + male + education + income + sisi_vote + ## not_voteboycott, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5577 -0.1762 -0.0834 0.0147 1.0161 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.07156 0.09466 0.76 0.450 ## treat_b1 0.08072 0.04658 1.73 0.084 . ## treat_b2 -0.01806 0.04630 -0.39 0.697 ## treat_b3 0.06037 0.04725 1.28 0.202 ## factor(age_cat)2 0.01238 0.03965 0.31 0.755 ## factor(age_cat)3 0.00815 0.04399 0.19 0.853 ## male -0.01509 0.04981 -0.30 0.762 ## education -0.00862 0.01171 -0.74 0.462 ## income 0.00479 0.01949 0.25 0.806 ## sisi_voteTRUE 0.43902 0.03863 11.37 &lt;2e-16 *** ## not_voteboycottTRUE 0.09667 0.04230 2.29 0.023 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.38 on 516 degrees of freedom ## (56 observations deleted due to missingness) ## Multiple R-squared: 0.23, Adjusted R-squared: 0.215 ## F-statistic: 15.4 on 10 and 516 DF, p-value: &lt;2e-16 6.6 Test whether the effects of treatment 1 (police-favorable message) are the same as treatment 3 (both sides) for each dependent variable. Feel free to use the linearHypothesis function in the car package. reg.1 &lt;- lm(just_binary_main~treat_b1 + treat_b2 + treat_b3 + factor(age_cat) + male + education + income + sisi_vote + not_voteboycott, data=dta) linearHypothesis(reg.1, &quot;treat_b1=treat_b3&quot;) ## Linear hypothesis test ## ## Hypothesis: ## treat_b1 - treat_b3 = 0 ## ## Model 1: restricted model ## Model 2: just_binary_main ~ treat_b1 + treat_b2 + treat_b3 + factor(age_cat) + ## male + education + income + sisi_vote + not_voteboycott ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 517 85.2 ## 2 516 84.1 1 1.17 7.21 0.0075 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 reg.2 &lt;- lm(not_acc_binary_main~treat_b1 + treat_b2 + treat_b3 + factor(age_cat) + male + education + income + sisi_vote + not_voteboycott, data=dta) linearHypothesis(reg.2, &quot;treat_b1=treat_b3&quot;) ## Linear hypothesis test ## ## Hypothesis: ## treat_b1 - treat_b3 = 0 ## ## Model 1: restricted model ## Model 2: not_acc_binary_main ~ treat_b1 + treat_b2 + treat_b3 + factor(age_cat) + ## male + education + income + sisi_vote + not_voteboycott ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 517 73.1 ## 2 516 73.0 1 0.0263 0.19 0.67 6.7 In some political contexts, messages have a different effect on people with less engagement in politics. Are there heterogeneous treatment effects by those who did not vote for reasons other than to boycott the election and those who did not? Feel free to explore multiple other ways there could be heterogeneous effects (e.g., perhaps there are different effects on Sisi supporters), but you only need to report on possible differential effects of the treatments on the “not-accountable” dependent variable. dta %&gt;% lm(not_acc_binary_main ~ not_voteboycott*treat_b1 + not_voteboycott*treat_b2 + not_voteboycott*treat_b3 +factor(age_cat) + male + education + income + sisi_vote, data = .) %&gt;% summary() ## ## Call: ## lm(formula = not_acc_binary_main ~ not_voteboycott * treat_b1 + ## not_voteboycott * treat_b2 + not_voteboycott * treat_b3 + ## factor(age_cat) + male + education + income + sisi_vote, ## data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5824 -0.2292 -0.0516 0.0338 0.9981 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.02227 0.09584 0.23 0.8164 ## not_voteboycottTRUE 0.25001 0.07586 3.30 0.0010 ** ## treat_b1 0.10557 0.05355 1.97 0.0492 * ## treat_b2 0.03852 0.05354 0.72 0.4722 ## treat_b3 0.14407 0.05405 2.67 0.0079 ** ## factor(age_cat)2 0.00943 0.03936 0.24 0.8108 ## factor(age_cat)3 0.01563 0.04372 0.36 0.7209 ## male -0.01769 0.04968 -0.36 0.7219 ## education -0.00751 0.01163 -0.65 0.5190 ## income 0.00628 0.01939 0.32 0.7460 ## sisi_voteTRUE 0.43424 0.03836 11.32 &lt;2e-16 *** ## not_voteboycottTRUE:treat_b1 -0.09147 0.10526 -0.87 0.3853 ## not_voteboycottTRUE:treat_b2 -0.21232 0.10454 -2.03 0.0428 * ## not_voteboycottTRUE:treat_b3 -0.33778 0.10839 -3.12 0.0019 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.37 on 513 degrees of freedom ## (56 observations deleted due to missingness) ## Multiple R-squared: 0.246, Adjusted R-squared: 0.227 ## F-statistic: 12.9 on 13 and 513 DF, p-value: &lt;2e-16 "],["lab-vi-lists-loops-functions-in-r.html", "7 Lab VI: Lists, Loops, &amp; Functions in R 7.1 Load data. Stock price data is in stocks2020.csv. Presidential prices are in USPres_2020_Price History By Market -Bulk.xlsx. 7.2 Create data frame with daily price data for Trump in 2020 7.3 Merge the stock price and Trump price data 7.4 Create a list of stock ticker names (do not include DJI as a ticker) 7.5 Create a function that calculates daily percent change. 7.6 Check that your function worked 7.7 Loop thru list of tickers and run regressions in which daily change in stock price is a function of change in DJIA and change in Trump price. Create a data frame that stores the coefficient, standard error and t-stat for the Trump variable for each stock. Include a column that has the stock ticker in that data frame as well. Show the first six rows of the data frame with the ticker and coefficient information.", " 7 Lab VI: Lists, Loops, &amp; Functions in R 7.0.1 Preperation ## Packages library(plyr) library(tidyverse) library(readxl) library(readr) library(lubridate) library(knitr) 7.1 Load data. Stock price data is in stocks2020.csv. Presidential prices are in USPres_2020_Price History By Market -Bulk.xlsx. ## Reading in stock data stk &lt;- read_csv(&quot;~/GOVT8002/Spring 2023/Lab VII/stocks2020.csv&quot;, col_names = TRUE) ## Reading in presidential market data pres_mkt &lt;- read_excel(&quot;~/GOVT8002/Spring 2023/Lab VII/USPres_2020_Price History By Market -Bulk.xlsx&quot;, col_names = TRUE) %&gt;% dplyr::rename(&quot;date&quot; = &quot;Date (ET)&quot;, &quot;name&quot; = &quot;Contract Name&quot;, &quot;price&quot; = &quot;Close Share Price&quot;) 7.2 Create data frame with daily price data for Trump in 2020 ## Creating daily price data for Trump trump2020 &lt;- pres_mkt %&gt;% filter(year(date) &gt; 2019 &amp; name == &quot;Donald Trump&quot;) %&gt;% dplyr::select(c(date, &quot;Trump&quot; = price)) 7.3 Merge the stock price and Trump price data ## Joining the two datasets stk_pres &lt;- left_join(stk, trump2020, by = &quot;date&quot;) 7.4 Create a list of stock ticker names (do not include DJI as a ticker) ## Creating list of ticker names stk_tickers &lt;- c(names(stk)[!names(stk) %in% c(&quot;date&quot;, &quot;DJI&quot;)]) ## Creating number of stocks stk_num &lt;- length(stk_tickers) 7.5 Create a function that calculates daily percent change. Use the dplyr::lag() function inside your function. Apply the function to price data in your data frame using the mutate_at() function. ## Price Change Function pct_change &lt;- function(x){ (x - dplyr::lag(x))/dplyr::lag(x) } ## Daily Change Dataset daily_df &lt;- stk_pres %&gt;% dplyr::mutate_at(c(&quot;DJI&quot;, &quot;Trump&quot;, stk_tickers), pct_change) %&gt;% dplyr::select(c(date, c(&quot;DJI&quot;, &quot;Trump&quot;, stk_tickers))) 7.6 Check that your function worked cbind(stk_pres$AAPL, daily_df$AAPL)[1:3,] ## [,1] [,2] ## [1,] 75 NA ## [2,] 74 -0.0097 ## [3,] 75 0.0080 7.7 Loop thru list of tickers and run regressions in which daily change in stock price is a function of change in DJIA and change in Trump price. Create a data frame that stores the coefficient, standard error and t-stat for the Trump variable for each stock. Include a column that has the stock ticker in that data frame as well. Show the first six rows of the data frame with the ticker and coefficient information. ## Preparing Data Frame to store results ols_results &lt;- data.frame(&quot;row&quot; = 1:stk_num, &quot;ticker&quot; = NA, &quot;coef&quot; = NA, &quot;se&quot; = NA, &quot;tStat&quot; = NA) ## Looping Though Regressions for(i in 1:stk_num){ daily_df$temp &lt;- unlist(daily_df[, i + 3]) ols.1 &lt;- lm(temp ~ DJI + Trump, data = daily_df) ols_results[i, &quot;ticker&quot;] &lt;- stk_tickers[i] ols_results[i, 3:5] &lt;- round(summary(ols.1)$coefficients[&quot;Trump&quot;, 1:3], 3) } ## Showing results head(ols_results) ## row ticker coef se tStat ## 1 1 AAPL -0.015 0.011 -1.304 ## 2 2 NFLX -0.036 0.016 -2.208 ## 3 3 AMZN -0.034 0.013 -2.643 ## 4 4 K -0.003 0.011 -0.259 ## 5 5 TSLA -0.004 0.031 -0.115 ## 6 6 LMT 0.000 0.011 -0.026 7.7.1 IF INTERESTED IN DOING MORE (not required or for credit): Use lapply() (list apply) to regress stock price on Trump and DJI for all stock tickers. ## Now with lapply() ols_results2 &lt;- lapply(stk_tickers, function(x){ daily_df$temp = unlist(daily_df[, which(names(daily_df) == x)]) ols.1 &lt;- lm(temp ~ DJI + Trump, data = daily_df) }) head(ols_results2) ## [[1]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.00189 0.98010 -0.01476 ## ## ## [[2]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.0019 0.5354 -0.0358 ## ## ## [[3]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.0021 0.5324 -0.0336 ## ## ## [[4]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.00033 0.41330 -0.00285 ## ## ## [[5]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.00954 1.04214 -0.00360 ## ## ## [[6]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.000279 0.833710 -0.000286 "],["lab-vii-regression-discontinuity-designs.html", "8 Lab VII: Regression Discontinuity Designs 8.1 Replicate the main result using the specification for Table 1. In this model, the RD allows for differential quadratic models above and below the threshold and also has dummy variables for birthdays. The data is limited to observations within 2 years of 21. 8.2 Estimate a series of models with slightly different specifications. First, try the above specification with only linear age variable, first fixing slope to be the same above and below the threshhold and then allowing slope to vary above and below the threshhold. 8.3 Now see what happens when you use a different window sizes. Feel free to experiment (but only need to report one specification.) 8.4 To figure out what is going on, let’s create some figures. First, let’s re-create Figure 1 in the paper. I provide the code to create a binned graph for the alcohol arrest rate. Your task is to add another arrest category to the plot. 8.5 Now create a figure based on a linear model. You can try different window sizes but need only report one. Do this for alcohol (and feel free to add other crimes, but not required.)", " 8 Lab VII: Regression Discontinuity Designs 8.0.1 Preparation ## Packages require(knitr) require(haven) require(dplyr) library(rdrobust) # Load data: data saved as object named &quot;dta&quot; load(&quot;~/GOVT8002/Spring 2023/Lab VIII/Ch11_Lab_AlcoholCrime.RData&quot;) Carpenter and Dobkin (2015) analyzes the relationship between alcohol and crime using a regression discontinuity design. In this lab we will replicate the results and then explore different specifications. As you will see, some reasonable alternative specifications yield quite different results. The goal here will be to see if we can make any progress in explaining why the results vary as we vary the specification. From the codebook: “The individual level arrest records are collapsed into arrest counts by age in days for each crime type in the SAS program”P01 Estimate Age Profile of Crime Rates.sas”. This program uses populations estimated from the census in “P00 Estimate Population Denominators.sas” to compute arrest rates per 10,000.” The main variables are all_r: number of arrrests across all categories. Our main dependent variable. Arrest data is also broken down for violent_r, property_r, ill_drugs_r and alcohol_r post a dumm variable indicating the individual is over 21 years of age linear: the number of years from being age 21 (that is, linear = 1.0 indicates 22 year olds. Negative values means the individual is under 21 and positive values indicate the individual is over than 21. square: the squared value of linear linear_post: interaction of linear and post square_post: interaction of square and post birthday dummies: e.g., birthday_19 is a dummy for a person’s 19th birthday and birthday_19_1 is a dummy for the day after a person’s 19th birthday. Use this code in your formula: + birthday_19 + birthday_19_1 + birthday_20 + birthday_20_1 + birthday_21 + birthday_21_1 + birthday_22 + birthday_22_1 + birthday_23 + birthday_23_1 See Carpenter, Christopher and Carlos Dobkin. 2015. The Minimum Legal Drinking Age and Crime. The Review of Economics and Statistics. 97:2, 521-524. 8.1 Replicate the main result using the specification for Table 1. In this model, the RD allows for differential quadratic models above and below the threshold and also has dummy variables for birthdays. The data is limited to observations within 2 years of 21. ## Basic RD model rd_basic &lt;- lm(all_r ~ post + linear, data=dta[abs(dta$linear) &lt;= 2, ]) summary(rd_basic) ## ## Call: ## lm(formula = all_r ~ post + linear, data = dta[abs(dta$linear) &lt;= ## 2, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -89.9 -22.3 -2.3 16.9 695.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1534.91 2.54 605.0 &lt;2e-16 *** ## post 76.89 4.54 16.9 &lt;2e-16 *** ## linear -48.79 1.96 -24.9 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 43 on 1458 degrees of freedom ## Multiple R-squared: 0.325, Adjusted R-squared: 0.324 ## F-statistic: 351 on 2 and 1458 DF, p-value: &lt;2e-16 ## Differing Slopes rd_basic &lt;- lm(all_r ~ post*linear, data=dta[abs(dta$linear) &lt;= 2, ]) summary(rd_basic) ## ## Call: ## lm(formula = all_r ~ post * linear, data = dta[abs(dta$linear) &lt;= ## 2, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -89.8 -19.5 -2.8 14.9 671.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1559.03 3.04 512.29 &lt;2e-16 *** ## post 76.76 4.30 17.86 &lt;2e-16 *** ## linear -24.71 2.63 -9.39 &lt;2e-16 *** ## post:linear -48.06 3.72 -12.92 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 41 on 1457 degrees of freedom ## Multiple R-squared: 0.394, Adjusted R-squared: 0.393 ## F-statistic: 316 on 3 and 1457 DF, p-value: &lt;2e-16 ## Model with covariates and vary rd_replicate &lt;- lm(all_r ~ post + linear + square + linear_post + square_post + birthday_19 + birthday_20 + birthday_20_1 + birthday_21 + birthday_21_1 + birthday_22 + birthday_22_1, data=dta[abs(dta$linear) &lt;= 2, ]) summary(rd_replicate) ## ## Call: ## lm(formula = all_r ~ post + linear + square + linear_post + square_post + ## birthday_19 + birthday_20 + birthday_20_1 + birthday_21 + ## birthday_21_1 + birthday_22 + birthday_22_1, data = dta[abs(dta$linear) &lt;= ## 2, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -83.0 -17.6 -0.8 15.9 216.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1543.07 2.92 527.74 &lt; 2e-16 *** ## post 91.21 4.15 22.00 &lt; 2e-16 *** ## linear -71.06 6.76 -10.51 &lt; 2e-16 *** ## square -23.73 3.27 -7.25 6.9e-13 *** ## linear_post -16.26 9.57 -1.70 0.09 . ## square_post 33.67 4.63 7.27 5.8e-13 *** ## birthday_19 287.42 26.40 10.89 &lt; 2e-16 *** ## birthday_20 408.59 26.27 15.55 &lt; 2e-16 *** ## birthday_20_1 217.60 26.27 8.28 2.7e-16 *** ## birthday_21 633.83 26.40 24.01 &lt; 2e-16 *** ## birthday_21_1 673.30 26.40 25.51 &lt; 2e-16 *** ## birthday_22 347.87 26.27 13.24 &lt; 2e-16 *** ## birthday_22_1 382.07 26.27 14.54 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26 on 1448 degrees of freedom ## Multiple R-squared: 0.754, Adjusted R-squared: 0.752 ## F-statistic: 371 on 12 and 1448 DF, p-value: &lt;2e-16 ## Diagnostic plot hist(dta$linear, main = &quot;Diagnostic Plot&quot;, xlab = &quot;Age&quot;, las=1) 8.2 Estimate a series of models with slightly different specifications. First, try the above specification with only linear age variable, first fixing slope to be the same above and below the threshhold and then allowing slope to vary above and below the threshhold. ## Additional Models RD.1 &lt;- lm(all_r ~ post + linear + birthday_19 + birthday_20 + birthday_20_1 + birthday_21 + birthday_21_1 + birthday_22 + birthday_22_1, data=dta[abs(dta$linear) &lt;= 2, ]) summary(RD.1) ## ## Call: ## lm(formula = all_r ~ post + linear + birthday_19 + birthday_20 + ## birthday_20_1 + birthday_21 + birthday_21_1 + birthday_22 + ## birthday_22_1, data = dta[abs(dta$linear) &lt;= 2, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -86.03 -20.19 -0.52 18.63 200.85 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1537.00 1.74 885.50 &lt; 2e-16 *** ## post 68.66 3.11 22.09 &lt; 2e-16 *** ## linear -45.49 1.34 -33.84 &lt; 2e-16 *** ## birthday_19 249.71 29.65 8.42 &lt; 2e-16 *** ## birthday_20 416.50 29.62 14.06 &lt; 2e-16 *** ## birthday_20_1 225.57 29.62 7.62 4.7e-14 *** ## birthday_21 662.46 29.65 22.34 &lt; 2e-16 *** ## birthday_21_1 701.82 29.65 23.67 &lt; 2e-16 *** ## birthday_22 344.62 29.62 11.63 &lt; 2e-16 *** ## birthday_22_1 378.75 29.62 12.79 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 30 on 1451 degrees of freedom ## Multiple R-squared: 0.687, Adjusted R-squared: 0.685 ## F-statistic: 353 on 9 and 1451 DF, p-value: &lt;2e-16 RD.2 &lt;- lm(all_r ~ post + linear + linear_post + birthday_19 + birthday_20 + birthday_20_1 + birthday_21 + birthday_21_1 + birthday_22 + birthday_22_1, data=dta[abs(dta$linear) &lt;= 2, ]) summary(RD.2) ## ## Call: ## lm(formula = all_r ~ post + linear + linear_post + birthday_19 + ## birthday_20 + birthday_20_1 + birthday_21 + birthday_21_1 + ## birthday_22 + birthday_22_1, data = dta[abs(dta$linear) &lt;= ## 2, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -85.70 -17.55 -0.79 15.90 222.68 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1558.89 1.99 785.09 &lt;2e-16 *** ## post 68.72 2.81 24.45 &lt;2e-16 *** ## linear -23.60 1.72 -13.73 &lt;2e-16 *** ## linear_post -43.78 2.43 -18.00 &lt;2e-16 *** ## birthday_19 271.60 26.84 10.12 &lt;2e-16 *** ## birthday_20 416.50 26.79 15.55 &lt;2e-16 *** ## birthday_20_1 225.51 26.79 8.42 &lt;2e-16 *** ## birthday_21 640.51 26.84 23.86 &lt;2e-16 *** ## birthday_21_1 679.93 26.84 25.33 &lt;2e-16 *** ## birthday_22 344.56 26.79 12.86 &lt;2e-16 *** ## birthday_22_1 378.75 26.79 14.14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27 on 1450 degrees of freedom ## Multiple R-squared: 0.744, Adjusted R-squared: 0.742 ## F-statistic: 421 on 10 and 1450 DF, p-value: &lt;2e-16 8.3 Now see what happens when you use a different window sizes. Feel free to experiment (but only need to report one specification.) ## Different window sizes RD.3 &lt;- lm(all_r ~ post + linear + linear_post, data=dta[abs(dta$linear) &lt;= 1, ]) summary(RD.3) ## ## Call: ## lm(formula = all_r ~ post + linear + linear_post, data = dta[abs(dta$linear) &lt;= ## 1, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -85.2 -21.4 -3.2 15.4 665.3 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1543.85 4.97 310.46 &lt; 2e-16 *** ## post 98.44 7.01 14.04 &lt; 2e-16 *** ## linear -53.88 8.60 -6.27 6.2e-10 *** ## linear_post -33.66 12.13 -2.77 0.0057 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 47 on 727 degrees of freedom ## Multiple R-squared: 0.221, Adjusted R-squared: 0.218 ## F-statistic: 68.7 on 3 and 727 DF, p-value: &lt;2e-16 RD.4 &lt;- lm(all_r ~ post + linear + linear_post, data=dta[abs(dta$linear) &lt;= 3, ]) summary(RD.4) ## ## Call: ## lm(formula = all_r ~ post + linear + linear_post, data = dta[abs(dta$linear) &lt;= ## 3, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -256.5 -26.5 1.3 30.7 671.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1629.58 3.64 448.25 &lt;2e-16 *** ## post 6.25 5.14 1.22 0.22 ## linear 54.93 2.10 26.19 &lt;2e-16 *** ## linear_post -127.40 2.96 -42.98 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60 on 2187 degrees of freedom ## Multiple R-squared: 0.471, Adjusted R-squared: 0.47 ## F-statistic: 648 on 3 and 2187 DF, p-value: &lt;2e-16 8.4 To figure out what is going on, let’s create some figures. First, let’s re-create Figure 1 in the paper. I provide the code to create a binned graph for the alcohol arrest rate. Your task is to add another arrest category to the plot. ## With rdrobust rdplot(dta$all_r , dta$linear, c=0, kernel = &quot;triangular&quot;, x.label = &quot;Year from Age Cutoff&quot;, y.label = &quot;Arrest Rate&quot;, title = &quot;Effect of Alcohol on Crime&quot;, binselect = &quot;es&quot;) # The original paper used 2 week intervals for bins, using the following to # generate a number indicating age in years grouped into 2 week intervals dta$age_fortnight = 21 + (14*floor(dta$days_to_21/14))/365 # Create a variable placing each observation in a &quot;bin&quot; dta$bin = rep(1:length(unique(dta$age_fortnight)), table(dta$age_fortnight)) # Use &quot;window&quot; to set how wide the window is above and below 21; e.g. window = 2 # limits years 19 to 23 window = 2 # Create a dataframe that contains average crime rates grouped by bin # Use the dplyr package to simplify data organization dta_bin &lt;- dta %&gt;% dplyr::select(days_to_21, all_r, property_r, age_fortnight, alcohol_r, bin) %&gt;% group_by(bin) # Quadratic model with varying parameters below and above threshhold # ALCOHOL ARREST RATES Alc.RD.Replicate.left = lm(alcohol_r ~ linear + square, dta[dta$linear &gt; -window &amp; dta$linear &lt;0, ]) summary(Alc.RD.Replicate.left) ## ## Call: ## lm(formula = alcohol_r ~ linear + square, data = dta[dta$linear &gt; ## -window &amp; dta$linear &lt; 0, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -45.57 -8.20 -1.26 7.19 204.45 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 394.19 1.67 236.50 &lt; 2e-16 *** ## linear -8.54 3.85 -2.22 0.027 * ## square -11.46 1.86 -6.15 1.3e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15 on 726 degrees of freedom ## Multiple R-squared: 0.265, Adjusted R-squared: 0.263 ## F-statistic: 131 on 2 and 726 DF, p-value: &lt;2e-16 Alc.RD.Replicate.right = lm(alcohol_r ~ linear + square, dta[dta$linear &lt; window &amp; dta$linear &gt;0, ]) summary(Alc.RD.Replicate.right) ## ## Call: ## lm(formula = alcohol_r ~ linear + square, data = dta[dta$linear &lt; ## window &amp; dta$linear &gt; 0, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.9 -10.0 -0.8 7.2 486.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 418.34 2.68 156.34 &lt; 2e-16 *** ## linear -22.03 6.18 -3.57 0.00039 *** ## square 7.14 2.99 2.39 0.01724 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 24 on 726 degrees of freedom ## Multiple R-squared: 0.0409, Adjusted R-squared: 0.0382 ## F-statistic: 15.5 on 2 and 726 DF, p-value: 2.65e-07 # Quadratic model with varying parameters below and above threshhold # PROPERTY ARREST RATES Prop.RD.Replicate.left = lm(property_r ~ linear + square, dta[dta$linear &gt; -window &amp; dta$linear &lt;0, ]) summary(Prop.RD.Replicate.left) ## ## Call: ## lm(formula = property_r ~ linear + square, data = dta[dta$linear &gt; ## -window &amp; dta$linear &lt; 0, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.50 -6.08 -0.02 5.92 28.63 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 224.35 1.01 222.52 &lt; 2e-16 *** ## linear -30.71 2.33 -13.19 &lt; 2e-16 *** ## square 8.66 1.13 7.68 5.1e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.1 on 726 degrees of freedom ## Multiple R-squared: 0.905, Adjusted R-squared: 0.904 ## F-statistic: 3.44e+03 on 2 and 726 DF, p-value: &lt;2e-16 Prop.RD.Replicate.right = lm(property_r ~ linear + square, dta[dta$linear &lt; window &amp; dta$linear &gt;0, ]) summary(Prop.RD.Replicate.right) ## ## Call: ## lm(formula = property_r ~ linear + square, data = dta[dta$linear &lt; ## window &amp; dta$linear &gt; 0, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.56 -5.28 -0.03 4.94 25.22 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 229.351 0.861 266.44 &lt; 2e-16 *** ## linear -29.618 1.988 -14.90 &lt; 2e-16 *** ## square 3.668 0.962 3.81 0.00015 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.7 on 726 degrees of freedom ## Multiple R-squared: 0.737, Adjusted R-squared: 0.736 ## F-statistic: 1.02e+03 on 2 and 726 DF, p-value: &lt;2e-16 # Create a # scatter plot plot(dta_bin$age_fortnight, dta_bin$alcohol_r, type = &quot;p&quot;, pch = 1, cex = 0.5, cex.main = 0.8, xlab = &quot;&quot;, ylab = &quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, ylim = c(180, 480)) axis(2, las = 1, tick = T, cex.axis = .8, mgp = c(2,.7,0)) axis(1, tick = T, at= seq(17, 25, by=1), labels =seq(17, 25, by=1),cex.axis = .8, mgp = c(2,.3,0)) mtext(&quot;Arrest rate&quot;, las = 1, side = 2, at = 510, line = -0.2, cex = 1) mtext(&quot;Age at time of arrest&quot;, side = 1, line = 1., cex = 1) # Add fitted lines from &quot;left&quot; and &quot;right&quot; models # ALCOHOL points(seq(21 - window, 21, by = 0.1), coef(Alc.RD.Replicate.left)[1] + coef(Alc.RD.Replicate.left)[2]*seq(-window, 0, by = 0.1) + coef(Alc.RD.Replicate.left)[3]*(seq(-window, 0, by = 0.1)^2), type = &#39;l&#39;, lwd = 2) points(seq(21, 21 + window, by = 0.1), coef(Alc.RD.Replicate.right)[1] + coef(Alc.RD.Replicate.right)[2]*seq(0, window, by = 0.1) + coef(Alc.RD.Replicate.right)[3]*(seq(0, window, by = 0.1)^2), type = &#39;l&#39;, lwd = 2) 8.5 Now create a figure based on a linear model. You can try different window sizes but need only report one. Do this for alcohol (and feel free to add other crimes, but not required.) # Create a dataframe that contains average crime rates grouped by bin, limited to the # window set by the window parameter window = 4 library(dplyr) # Linear model with varying slopes below and above threshold # ALCOHOL ARREST RATES Alc.RD.linear.left = lm(alcohol_r ~ linear, dta[dta$linear &gt; -window &amp; dta$linear &lt;0, ]) summary(Alc.RD.linear.left) ## ## Call: ## lm(formula = alcohol_r ~ linear, data = dta[dta$linear &gt; -window &amp; ## dta$linear &lt; 0, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.86 -31.18 0.58 28.79 207.23 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 451.961 1.821 248.3 &lt;2e-16 *** ## linear 63.473 0.788 80.5 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35 on 1457 degrees of freedom ## Multiple R-squared: 0.816, Adjusted R-squared: 0.816 ## F-statistic: 6.48e+03 on 1 and 1457 DF, p-value: &lt;2e-16 Alc.RD.linear.right = lm(alcohol_r ~ linear, dta[dta$linear &lt; window &amp; dta$linear &gt;0, ]) summary(Alc.RD.linear.right) ## ## Call: ## lm(formula = alcohol_r ~ linear, data = dta[dta$linear &lt; window &amp; ## dta$linear &gt; 0, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.8 -9.2 -1.1 7.1 486.9 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 417.48 1.11 376.3 &lt;2e-16 *** ## linear -11.31 0.48 -23.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 21 on 1457 degrees of freedom ## Multiple R-squared: 0.276, Adjusted R-squared: 0.275 ## F-statistic: 555 on 1 and 1457 DF, p-value: &lt;2e-16 # Create a scatter plot plot(dta_bin$age_fortnight, dta_bin$alcohol_r, type = &quot;p&quot;, pch = 1, cex = 0.5, cex.main = 0.8, xlab = &quot;&quot;, ylab = &quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;) axis(2, las = 1, tick = T, cex.axis = .8, mgp = c(2,.7,0)) axis(1, tick = T, at= seq(17, 25, by=1), labels =seq(17, 25, by=1),cex.axis = .8, mgp = c(2,.3,0)) mtext(&quot;Arrest rate&quot;, las = 1, side = 2, at = 520, line = -0.2, cex = 1) mtext(&quot;Age at time of arrest&quot;, side = 1, line = 1., cex = 0.8) # Add fitted lines from &quot;left&quot; and &quot;right&quot; models points(seq(21 - window, 21, by = 0.1), coef(Alc.RD.linear.left) [1] + coef(Alc.RD.linear.left) [2]*seq(-window, 0, by = 0.1), lwd = 2, col = &quot;darkblue&quot;, type = &#39;l&#39;) points(seq(21, 21 + window, by = 0.1), coef(Alc.RD.linear.right)[1] + coef(Alc.RD.linear.right)[2]*seq(0, window , by = 0.1), lwd = 2, col = &quot;darkblue&quot;, type = &#39;l&#39;) "],["lab-viii-time-series.html", "9 Lab VIII: Time Series 9.1 Preparation 9.2 Data description: 9.3 Estimate a model with the log of miles driven (use the unadjusted data: logMilesNA) as a function of log gas prices, unemployment and log of population. (Don’t account for autocorrelation.) Is the effect of gas statistically significant? 9.4 Do you think there is seasonality in driving? Test by adding dummy variables for month to the above model. Is there evidence of monthly variation? Is the effect of gas statistically significant? Now create a categorical variable for seasons and run the model without the month dummy variable. 9.5 Briefly explain why there might be autocorrelation. 9.6 Create a figure that is useful to assess whether there is autocorrelation (you have two choices here). Draw a sketch here. 9.7 Test whether there is first order autocorrelation. Report the key statistic from this test. What would a coefficient of 1 indicate? 9.8 Use Newey-West standard errors to account for autocorrelation. Use the same variables as in part (b). Determine the t-stats and describe the similarities and difference with earlier results. 9.9 Estimate a model that adjusts for autocorrelation. Use the same variables as in part (b). Describe similarities and difference with earlier results. 9.10 Estimate a dynamic model of miles driven using control variables from above. Discuss key differences. 9.11 Run an Augmented Dickey-Fuller Test on logMilesNA. 9.12 OPTIONAL QUESTION: Conduct the same analysis from a, b, d, and g using light truck sales as the dependent variable. (Light truck sales use more gas than cars, so the question is whether gas prices affect the kind of car people buy which will affect gas consumption for the life of the car.)", " 9 Lab VIII: Time Series 9.1 Preparation ## Setup Options options(digits = 3) ## Packages library(haven) library(AER) library(orcutt) library(tidyverse) library(lubridate) library(lmtest) library(sandwich) ## Loading Data load(&quot;~/GOVT8002/Spring 2023/Lab IX/Ch13_Lab_GasPrices.RData&quot;) 9.2 Data description: Variable Description year Year month Month time Time identifier (1 for first observation, etc) logMilesNA Log of vehicle miles traveled (in millions) in the U.S., not seasonally adjusted logLightTruckSales Log of retail sales of light weight trucks in the U.S. (in thousands) logGasReal Log of real price of gas unem Unemployment rate logPop Log of U.S. population (in thousands) 9.3 Estimate a model with the log of miles driven (use the unadjusted data: logMilesNA) as a function of log gas prices, unemployment and log of population. (Don’t account for autocorrelation.) Is the effect of gas statistically significant? ## Model 1 reg.1 &lt;- dta %&gt;% lm(logMilesNA ~ logGasReal + unem + logPop, data = .) ## Summary Output summary(reg.1) ## ## Call: ## lm(formula = logMilesNA ~ logGasReal + unem + logPop, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.19523 -0.04573 0.00491 0.05196 0.13648 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.91875 1.06539 -5.56 6.1e-08 *** ## logGasReal 0.03039 0.02115 1.44 0.15 ## unem -0.01979 0.00265 -7.46 9.7e-13 *** ## logPop 1.46174 0.08459 17.28 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0683 on 300 degrees of freedom ## (260 observations deleted due to missingness) ## Multiple R-squared: 0.724, Adjusted R-squared: 0.722 ## F-statistic: 263 on 3 and 300 DF, p-value: &lt;2e-16 ## 95% CI confint(reg.1) ## 2.5 % 97.5 % ## (Intercept) -8.0153 -3.8222 ## logGasReal -0.0112 0.0720 ## unem -0.0250 -0.0146 ## logPop 1.2953 1.6282 9.4 Do you think there is seasonality in driving? Test by adding dummy variables for month to the above model. Is there evidence of monthly variation? Is the effect of gas statistically significant? Now create a categorical variable for seasons and run the model without the month dummy variable. ## Removing NAs dta &lt;- dta %&gt;% drop_na() ## Creating a variable for seasons dta &lt;- dta %&gt;% mutate(season = case_when( month == 1 | month == 2 | month == 3 ~ &quot;Winter&quot;, month == 4 | month == 5 | month == 6 ~ &quot;Spring&quot;, month == 7 | month == 8 | month == 9 ~ &quot;Summer&quot;, month == 10 | month == 11 | month == 12 ~ &quot;Fall&quot;) ) ## Model 2 Months reg.2a &lt;- dta %&gt;% lm(logMilesNA ~ logGasReal + unem + logPop + factor(month), data = .) ## Summary Output summary(reg.2a) ## ## Call: ## lm(formula = logMilesNA ~ logGasReal + unem + logPop + factor(month), ## data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.07837 -0.01709 0.00095 0.01992 0.05803 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.43243 0.44362 -19.01 &lt; 2e-16 *** ## logGasReal -0.04211 0.00896 -4.70 4.0e-06 *** ## unem -0.01414 0.00112 -12.60 &lt; 2e-16 *** ## logPop 1.65186 0.03517 46.97 &lt; 2e-16 *** ## factor(month)2 -0.04746 0.00791 -6.00 5.8e-09 *** ## factor(month)3 0.09982 0.00793 12.58 &lt; 2e-16 *** ## factor(month)4 0.09338 0.00803 11.63 &lt; 2e-16 *** ## factor(month)5 0.14264 0.00806 17.69 &lt; 2e-16 *** ## factor(month)6 0.14402 0.00800 18.00 &lt; 2e-16 *** ## factor(month)7 0.16943 0.00798 21.23 &lt; 2e-16 *** ## factor(month)8 0.16520 0.00801 20.63 &lt; 2e-16 *** ## factor(month)9 0.07426 0.00798 9.31 &lt; 2e-16 *** ## factor(month)10 0.11323 0.00796 14.22 &lt; 2e-16 *** ## factor(month)11 0.04218 0.00791 5.33 2.0e-07 *** ## factor(month)12 0.05098 0.00789 6.46 4.4e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0279 on 289 degrees of freedom ## Multiple R-squared: 0.956, Adjusted R-squared: 0.953 ## F-statistic: 444 on 14 and 289 DF, p-value: &lt;2e-16 ## 95% CI confint(reg.2a) ## 2.5 % 97.5 % ## (Intercept) -9.3056 -7.5593 ## logGasReal -0.0598 -0.0245 ## unem -0.0163 -0.0119 ## logPop 1.5826 1.7211 ## factor(month)2 -0.0630 -0.0319 ## factor(month)3 0.0842 0.1154 ## factor(month)4 0.0776 0.1092 ## factor(month)5 0.1268 0.1585 ## factor(month)6 0.1283 0.1598 ## factor(month)7 0.1537 0.1851 ## factor(month)8 0.1494 0.1810 ## factor(month)9 0.0586 0.0900 ## factor(month)10 0.0976 0.1289 ## factor(month)11 0.0266 0.0578 ## factor(month)12 0.0354 0.0665 ## Model 2 Seasons reg.2b &lt;- dta %&gt;% lm(logMilesNA ~ logGasReal + unem + logPop + factor(season), data = .) ## Summary Output summary(reg.2b) ## ## Call: ## lm(formula = logMilesNA ~ logGasReal + unem + logPop + factor(season), ## data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.13922 -0.04079 0.00449 0.03595 0.14090 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.83213 0.80624 -9.71 &lt; 2e-16 *** ## logGasReal -0.02693 0.01624 -1.66 0.098 . ## unem -0.01451 0.00203 -7.14 7.3e-12 *** ## logPop 1.60983 0.06396 25.17 &lt; 2e-16 *** ## factor(season)Spring 0.05684 0.00834 6.82 5.1e-11 *** ## factor(season)Summer 0.06578 0.00829 7.93 4.5e-14 *** ## factor(season)Winter -0.05089 0.00842 -6.04 4.6e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.051 on 297 degrees of freedom ## Multiple R-squared: 0.848, Adjusted R-squared: 0.845 ## F-statistic: 275 on 6 and 297 DF, p-value: &lt;2e-16 ## 95% CI confint(reg.2b) ## 2.5 % 97.5 % ## (Intercept) -9.4188 -6.24546 ## logGasReal -0.0589 0.00502 ## unem -0.0185 -0.01051 ## logPop 1.4839 1.73571 ## factor(season)Spring 0.0404 0.07324 ## factor(season)Summer 0.0495 0.08211 ## factor(season)Winter -0.0675 -0.03432 9.5 Briefly explain why there might be autocorrelation. With 50 years of data, there are certainly time trends and we can reasonably expect the error term of one year to be correlated with the error term of the previous year. For example, if more people began longer commutes in the 1990s, the longer commutes (more miles driven) will continue into the 2000s so the errors will be correlated year by year. 9.6 Create a figure that is useful to assess whether there is autocorrelation (you have two choices here). Draw a sketch here. ## Saving the Residuals dta$residual_errors &lt;- resid(reg.2a) ## Regressing Residuals lag_err &lt;- c(NA, dta$residual_errors[1:(length(dta$residual_errors)-1)]) plot(lag_err, dta$residual_errors, pch=20, xlab = &quot;Lagged Residuals&quot;, ylab = &quot;Residuals&quot;, bty=&quot;n&quot;, col=&quot;dark blue&quot;, las=1) abline(lm(dta$residual_errors ~ lag_err), lwd=2) ## Plotting Over Time plot(dta$residual_errors, pch=20, xlab = &quot;Time&quot;, las=1, ylab = &quot;Residuals&quot;, bty=&quot;n&quot;, col=&quot;dark blue&quot;) # Creating season category and plotting residual error dta %&gt;% ggplot() + geom_line(aes(x=as.Date(date), y=residual_errors, col=season), size=.8) + scale_color_manual(values=c(&quot;orange&quot;, &quot;green&quot;, &quot;red&quot;, &quot;darkblue&quot;))+ labs(x=&quot;Date&quot;, y=&quot;Residual Errors&quot;) + theme_classic() + theme(panel.grid.major.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;), panel.grid.minor.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;)) %&gt;% labs(col=&quot;Season&quot;) # Creating season category and plotting residual error dta %&gt;% ggplot() + geom_point(aes(x=as.Date(date), y=residual_errors, col=season)) + scale_color_manual(values=c(&quot;orange&quot;, &quot;green&quot;, &quot;red&quot;, &quot;darkblue&quot;))+ labs(x=&quot;Date&quot;, y=&quot;Residual Errors&quot;) + theme_classic() + theme(panel.grid.major.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;), panel.grid.minor.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;)) %&gt;% labs(col=&quot;Season&quot;) 9.7 Test whether there is first order autocorrelation. Report the key statistic from this test. What would a coefficient of 1 indicate? ## Error on Lagged Error Regression reg.lag &lt;- lm(dta$residual_errors ~ lag_err) summary(reg.lag) ## ## Call: ## lm(formula = dta$residual_errors ~ lag_err) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.07419 -0.01022 0.00114 0.01039 0.06575 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.06e-05 8.99e-04 -0.05 0.96 ## lag_err 8.21e-01 3.30e-02 24.89 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0157 on 301 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.673, Adjusted R-squared: 0.672 ## F-statistic: 620 on 1 and 301 DF, p-value: &lt;2e-16 ## Durbin-Watson dwtest(reg.2a) ## ## Durbin-Watson test ## ## data: reg.2a ## DW = 0.4, p-value &lt;2e-16 ## alternative hypothesis: true autocorrelation is greater than 0 9.8 Use Newey-West standard errors to account for autocorrelation. Use the same variables as in part (b). Determine the t-stats and describe the similarities and difference with earlier results. ## Calculating Newey West Standard Errors sqrt(diag(NeweyWest(reg.2a, lag=4, prewhite=FALSE, adjust=TRUE))) ## (Intercept) logGasReal unem logPop factor(month)2 factor(month)3 factor(month)4 ## 1.03863 0.01780 0.00185 0.08261 0.00595 0.00699 0.00812 ## factor(month)5 factor(month)6 factor(month)7 factor(month)8 factor(month)9 factor(month)10 factor(month)11 ## 0.00830 0.00873 0.00872 0.00893 0.00868 0.00799 0.00764 ## factor(month)12 ## 0.00648 ## Checking t-stats coef(reg.2a) / sqrt(diag(NeweyWest(reg.2a, lag=4, prewhite=FALSE, adjust=TRUE))) ## (Intercept) logGasReal unem logPop factor(month)2 factor(month)3 factor(month)4 ## -8.12 -2.37 -7.63 20.00 -7.97 14.28 11.50 ## factor(month)5 factor(month)6 factor(month)7 factor(month)8 factor(month)9 factor(month)10 factor(month)11 ## 17.19 16.49 19.43 18.51 8.56 14.16 5.52 ## factor(month)12 ## 7.87 9.9 Estimate a model that adjusts for autocorrelation. Use the same variables as in part (b). Describe similarities and difference with earlier results. ## Cochrane-Orcutt Model reg.orc &lt;- cochrane.orcutt(reg.2a) summary(reg.orc) ## Call: ## lm(formula = logMilesNA ~ logGasReal + unem + logPop + factor(month), ## data = .) ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.56609 1.16030 -7.38 1.7e-12 *** ## logGasReal -0.05457 0.01653 -3.30 0.00108 ** ## unem -0.01067 0.00302 -3.53 0.00049 *** ## logPop 1.66049 0.09229 17.99 &lt; 2e-16 *** ## factor(month)2 -0.04674 0.00333 -14.05 &lt; 2e-16 *** ## factor(month)3 0.10185 0.00461 22.09 &lt; 2e-16 *** ## factor(month)4 0.09775 0.00594 16.46 &lt; 2e-16 *** ## factor(month)5 0.14735 0.00642 22.97 &lt; 2e-16 *** ## factor(month)6 0.14733 0.00620 23.75 &lt; 2e-16 *** ## factor(month)7 0.17257 0.00617 27.97 &lt; 2e-16 *** ## factor(month)8 0.16921 0.00628 26.93 &lt; 2e-16 *** ## factor(month)9 0.07941 0.00631 12.58 &lt; 2e-16 *** ## factor(month)10 0.11843 0.00600 19.74 &lt; 2e-16 *** ## factor(month)11 0.04669 0.00521 8.96 &lt; 2e-16 *** ## factor(month)12 0.05494 0.00422 13.02 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0157 on 298 degrees of freedom ## Multiple R-squared: 0.937 , Adjusted R-squared: 0.936 ## F-statistic: 305 on 4 and 298 DF, p-value: &lt; 1.79e-163 ## ## Durbin-Watson statistic ## (original): 0.35861 , p-value: 5.127e-46 ## (transformed): 2.58848 , p-value: 1e+00 ## &quot;By Hand&quot; reg.2a &lt;- dta %&gt;% lm(logMilesNA ~ logGasReal + unem + logPop + factor(month), data = .) ## Error on Lagged Error Regression reg.lag &lt;- lm(dta$residual_errors ~ lag_err) rho &lt;- summary(reg.lag)$coef[2] ## Lagging Each Variable dta$lagmile &lt;- dplyr::lag(dta$logMilesNA) dta$lag_gas &lt;- lag(dta$logGasReal) dta$lag_unem &lt;- lag(dta$unem) dta$lag_pop &lt;- lag(dta$logPop) dta$lag_month &lt;- lag(dta$month) ## Rho-transforming rho_mile &lt;- dta$logMilesNA - rho*dta$lagmile rho_gas &lt;- dta$logGasReal - rho*dta$lag_gas rho_unem &lt;- dta$unem - rho*dta$lag_unem rho_pop &lt;- dta$logPop - rho*dta$lag_pop rho_month &lt;- dta$month - rho*dta$lag_month ## Running rho-transformed model rho_trans_model &lt;- lm(rho_mile ~ rho_gas + rho_unem + rho_pop + factor(rho_month)) summary(rho_trans_model) ## ## Call: ## lm(formula = rho_mile ~ rho_gas + rho_unem + rho_pop + factor(rho_month)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.07137 -0.00993 0.00082 0.01017 0.06405 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.78822 0.25546 -7.00 1.8e-11 *** ## rho_gas -0.08278 0.02873 -2.88 0.00425 ** ## rho_unem -0.01297 0.00357 -3.63 0.00033 *** ## rho_pop 1.75410 0.11344 15.46 &lt; 2e-16 *** ## factor(rho_month)0.357014851611262 0.00606 0.00449 1.35 0.17854 ## factor(rho_month)0.535522277416893 0.19274 0.00451 42.77 &lt; 2e-16 *** ## factor(rho_month)0.714029703222524 0.06925 0.00456 15.18 &lt; 2e-16 *** ## factor(rho_month)0.892537129028154 0.11809 0.00458 25.77 &lt; 2e-16 *** ## factor(rho_month)1.07104455483379 0.07534 0.00455 16.57 &lt; 2e-16 *** ## factor(rho_month)1.24955198063942 0.10503 0.00453 23.16 &lt; 2e-16 *** ## factor(rho_month)1.42805940644505 0.08256 0.00455 18.15 &lt; 2e-16 *** ## factor(rho_month)1.60656683225068 -0.00474 0.00457 -1.04 0.29980 ## factor(rho_month)1.78507425805631 0.10822 0.00452 23.93 &lt; 2e-16 *** ## factor(rho_month)1.96358168386194 0.00279 0.00450 0.62 0.53575 ## factor(rho_month)2.14208910966757 0.07001 0.00448 15.61 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0159 on 288 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.938, Adjusted R-squared: 0.935 ## F-statistic: 310 on 14 and 288 DF, p-value: &lt;2e-16 9.10 Estimate a dynamic model of miles driven using control variables from above. Discuss key differences. ## Distributed Lag Model ## Creating lagged miles variable reg.2.dyn &lt;- lm(logMilesNA ~ lagmile + logGasReal + unem + logPop + factor(month) , data=dta) summary(reg.2.dyn) ## ## Call: ## lm(formula = logMilesNA ~ lagmile + logGasReal + unem + logPop + ## factor(month), data = dta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.07062 -0.01014 0.00093 0.01042 0.06385 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.909804 0.367611 -5.20 3.9e-07 *** ## lagmile 0.806402 0.032767 24.61 &lt; 2e-16 *** ## logGasReal -0.015273 0.005246 -2.91 0.0039 ** ## unem -0.002533 0.000794 -3.19 0.0016 ** ## logPop 0.337675 0.057044 5.92 9.2e-09 *** ## factor(month)2 0.005074 0.004979 1.02 0.3090 ## factor(month)3 0.191034 0.005840 32.71 &lt; 2e-16 *** ## factor(month)4 0.069694 0.004669 14.93 &lt; 2e-16 *** ## factor(month)5 0.118536 0.004691 25.27 &lt; 2e-16 *** ## factor(month)6 0.076598 0.005313 14.42 &lt; 2e-16 *** ## factor(month)7 0.106210 0.005216 20.36 &lt; 2e-16 *** ## factor(month)8 0.084078 0.005622 14.96 &lt; 2e-16 *** ## factor(month)9 -0.003285 0.005560 -0.59 0.5552 ## factor(month)10 0.108316 0.004533 23.89 &lt; 2e-16 *** ## factor(month)11 0.003511 0.004769 0.74 0.4622 ## factor(month)12 0.069662 0.004554 15.30 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0159 on 287 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.986, Adjusted R-squared: 0.985 ## F-statistic: 1.31e+03 on 15 and 287 DF, p-value: &lt;2e-16 ## Saving the Residuals residual_errors &lt;- resid(reg.2.dyn) ## Regressing Residuals lag_err &lt;- c(NA, residual_errors[1:(length(residual_errors)-1)]) ## Residuals from Dynamic Model plot(lag_err, residual_errors, pch=20, xlab = &quot;Lagged Residuals&quot;, ylab = &quot;Residuals&quot;, bty=&quot;n&quot;, col=&quot;dark blue&quot;, las=1) 9.11 Run an Augmented Dickey-Fuller Test on logMilesNA. ## Change in miles delta_miles &lt;- dta$logMilesNA - dta$lagmile ## Lagged Change in Miles lag_delta_miles &lt;- c(NA, delta_miles[1:(length(dta$lagmile)-1)]) ## Augmented Dickey-Fuller Test aug_dickey_fuller &lt;- lm(delta_miles ~ dta$lagmile + dta$year + lag_delta_miles) summary(aug_dickey_fuller) ## ## Call: ## lm(formula = delta_miles ~ dta$lagmile + dta$year + lag_delta_miles) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.1406 -0.0549 0.0122 0.0375 0.1330 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.294973 1.160772 -3.70 0.00026 *** ## dta$lagmile -0.280021 0.045474 -6.16 2.4e-09 *** ## dta$year 0.003870 0.000782 4.95 1.3e-06 *** ## lag_delta_miles -0.145002 0.056904 -2.55 0.01133 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.058 on 298 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.183, Adjusted R-squared: 0.175 ## F-statistic: 22.3 on 3 and 298 DF, p-value: 4.71e-13 9.12 OPTIONAL QUESTION: Conduct the same analysis from a, b, d, and g using light truck sales as the dependent variable. (Light truck sales use more gas than cars, so the question is whether gas prices affect the kind of car people buy which will affect gas consumption for the life of the car.) ## The Model reg.lt &lt;- lm(lttrucksales~logGasReal + unem + logPop + factor(month) , data=dta) summary(reg.lt) ## ## Call: ## lm(formula = lttrucksales ~ logGasReal + unem + logPop + factor(month), ## data = dta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -226.0 -44.3 1.0 37.4 360.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -21060.03 1197.03 -17.59 &lt; 2e-16 *** ## logGasReal -137.90 24.18 -5.70 2.9e-08 *** ## unem -58.14 3.03 -19.21 &lt; 2e-16 *** ## logPop 1744.24 94.89 18.38 &lt; 2e-16 *** ## factor(month)2 60.00 21.34 2.81 0.0053 ** ## factor(month)3 164.63 21.41 7.69 2.3e-13 *** ## factor(month)4 87.06 21.67 4.02 7.5e-05 *** ## factor(month)5 148.80 21.76 6.84 4.8e-11 *** ## factor(month)6 160.84 21.59 7.45 1.1e-12 *** ## factor(month)7 142.04 21.54 6.60 2.0e-10 *** ## factor(month)8 137.30 21.60 6.36 8.1e-10 *** ## factor(month)9 56.15 21.52 2.61 0.0096 ** ## factor(month)10 53.67 21.48 2.50 0.0130 * ## factor(month)11 19.99 21.36 0.94 0.3500 ## factor(month)12 108.29 21.30 5.08 6.6e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 75.4 on 289 degrees of freedom ## Multiple R-squared: 0.772, Adjusted R-squared: 0.761 ## F-statistic: 70 on 14 and 289 DF, p-value: &lt;2e-16 ## Saving the Residuals dta$residual_errors &lt;- resid(reg.lt) ## Regressing Residuals lag_err &lt;- c(NA, dta$residual_errors[1:(length(dta$residual_errors)-1)]) plot(lag_err, dta$residual_errors, pch=20, xlab = &quot;Lagged Residuals&quot;, ylab = &quot;Residuals&quot;, bty=&quot;n&quot;, col=&quot;dark blue&quot;, las=1) abline(lm(dta$residual_errors ~ lag_err), lwd=2) ## Plotting Over Time plot(dta$residual_errors, pch=20, xlab = &quot;Time&quot;, las=1, ylab = &quot;Residuals&quot;, bty=&quot;n&quot;, col=&quot;dark blue&quot;) # Creating season category and plotting residual error dta %&gt;% ggplot() + geom_line(aes(x=as.Date(date), y=residual_errors, col=season), size=.8) + scale_color_manual(values=c(&quot;orange&quot;, &quot;green&quot;, &quot;red&quot;, &quot;darkblue&quot;))+ labs(x=&quot;Date&quot;, y=&quot;Residual Errors&quot;) + theme_classic() + theme(panel.grid.major.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;), panel.grid.minor.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;)) %&gt;% labs(col=&quot;Season&quot;) # Creating season category and plotting residual error dta %&gt;% ggplot() + geom_point(aes(x=as.Date(date), y=residual_errors, col=season)) + scale_color_manual(values=c(&quot;orange&quot;, &quot;green&quot;, &quot;red&quot;, &quot;darkblue&quot;))+ labs(x=&quot;Date&quot;, y=&quot;Residual Errors&quot;) + theme_classic() + theme(panel.grid.major.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;), panel.grid.minor.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;)) %&gt;% labs(col=&quot;Season&quot;) ## Cochrane-Orcutt Model reg.orc2 &lt;- cochrane.orcutt(reg.lt) summary(reg.orc2) ## Call: ## lm(formula = lttrucksales ~ logGasReal + unem + logPop + factor(month), ## data = dta) ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -21271.5 2398.4 -8.87 &lt; 2e-16 *** ## logGasReal -148.4 44.3 -3.35 0.00092 *** ## unem -52.5 6.2 -8.47 1.3e-15 *** ## logPop 1758.2 190.3 9.24 &lt; 2e-16 *** ## factor(month)2 60.0 12.9 4.66 4.8e-06 *** ## factor(month)3 165.7 16.8 9.87 &lt; 2e-16 *** ## factor(month)4 91.0 19.7 4.61 6.0e-06 *** ## factor(month)5 152.7 21.0 7.26 3.6e-12 *** ## factor(month)6 162.2 20.9 7.76 1.4e-13 *** ## factor(month)7 143.1 20.8 6.87 3.9e-11 *** ## factor(month)8 139.7 20.9 6.67 1.3e-10 *** ## factor(month)9 60.0 20.8 2.89 0.00409 ** ## factor(month)10 57.9 19.7 2.94 0.00351 ** ## factor(month)11 23.4 17.4 1.34 0.18114 ## factor(month)12 111.1 13.9 7.98 3.5e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 57.3 on 298 degrees of freedom ## Multiple R-squared: 0.631 , Adjusted R-squared: 0.626 ## F-statistic: 35.1 on 4 and 298 DF, p-value: &lt; 4.603e-54 ## ## Durbin-Watson statistic ## (original): 0.72932 , p-value: 1.646e-28 ## (transformed): 2.33482 , p-value: 9.971e-01 "],["lab-ix-binary-response-models.html", "10 Lab IX: Binary Response Models 10.1 Use a LPM to estimate the effect of passenger class on survival. 10.2 Assess the following: did being a women or child affected survival? Did boarding location (a rough proxy for country of origin) affect survival? Ireland is Queenstown (“Q”), France is Cherbourg (“C”) and the Englad is Southampton (“S”). Treat Southampton as the reference category. Control for age in your model. 10.3 For the model from above, what are the minimum and maximum predicted probabilities of survival? 10.4 What is the name, age, gender and passenger class of the person with the lowest probability of surviving? 10.5 What is the name of the person with the highest probability of surviving? 10.6 Estimate a probit model where survival is a function of (only) passenger class. Treat passenger class as a nominal variable. Compare statistical significance to a similar LPM model. Is there an easy way to interpret the coefficients? 10.7 Estimate a probit model where survival is a function of passenger class (treated as a nominal variable) age, gender, child and embarkation location. What is the minimum and maximum fitted value? 10.8 What is the name of the person with the lowest probability of surviving? 10.9 For the above model, what is the effect of growing one year older (for an adult)? (Do this “manually”, using the observed-value, discrete difference method described in the book/lecture.) 10.10 Compare the probit effect of age to the LPM effect of age in part (b) 10.11 What is the effect of the passenger class, female and child variables in the above probit model? Use the mfx package as described in the lecture. Compare the predicted effects of these variables in the probit model to the results in the LPM in part (b). You need only discuss one of these variables, but please note all of them as you do the work.", " 10 Lab IX: Binary Response Models 10.0.1 Preparation ## Packages You May Need library(knitr) library(haven) ## install.packages(&quot;haven&quot;) library(car) ## install.packages(&quot;car&quot;) library(AER) ## install.packages(&quot;AER&quot;) library(Hmisc) ## use the describe command library(mfx) ## for marginal effects calculation library(tidyverse) ## Loading Data dta &lt;- read_dta(&quot;~/GOVT8002/Spring 2023/Lab X/Ch12_Lab_Titanic.dta&quot;) 10.1 Use a LPM to estimate the effect of passenger class on survival. ## Creating Variables dta$pclass_2 &lt;- (dta$pclass == 2) dta$pclass_3 &lt;- (dta$pclass == 3) ## LPM Model lpm_1 &lt;- lm(survived ~ pclass_2 + pclass_3, data = dta) summary(lpm_1) ## ## Call: ## lm(formula = survived ~ pclass_2 + pclass_3, data = dta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.619 -0.255 -0.255 0.381 0.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.6192 0.0257 24.08 &lt; 2e-16 *** ## pclass_2TRUE -0.1896 0.0378 -5.01 6.2e-07 *** ## pclass_3TRUE -0.3639 0.0310 -11.73 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.462 on 1306 degrees of freedom ## Multiple R-squared: 0.0977, Adjusted R-squared: 0.0963 ## F-statistic: 70.7 on 2 and 1306 DF, p-value: &lt;2e-16 ## Can also use factor(pclass) 10.2 Assess the following: did being a women or child affected survival? Did boarding location (a rough proxy for country of origin) affect survival? Ireland is Queenstown (“Q”), France is Cherbourg (“C”) and the Englad is Southampton (“S”). Treat Southampton as the reference category. Control for age in your model. ## Creating Variables dta$female &lt;- (dta$sex == &quot;female&quot;) dta$child &lt;- (dta$age &lt; 17) dta$Queensland &lt;- (dta$embarked == &quot;Q&quot;) dta$Cherbourg &lt;- (dta$embarked == &quot;C&quot;) ## LPM Model lpm_2 &lt;- lm(survived ~ pclass_2 + pclass_3 + female + child + Queensland + Cherbourg + age, data = dta) ## Summary Output summary(lpm_2) ## ## Call: ## lm(formula = survived ~ pclass_2 + pclass_3 + female + child + ## Queensland + Cherbourg + age, data = dta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0699 -0.2387 -0.0725 0.2427 1.0243 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.50200 0.05490 9.14 &lt; 2e-16 *** ## pclass_2TRUE -0.16197 0.03670 -4.41 1.1e-05 *** ## pclass_3TRUE -0.31691 0.03451 -9.18 &lt; 2e-16 *** ## femaleTRUE 0.48862 0.02549 19.17 &lt; 2e-16 *** ## childTRUE 0.08680 0.04448 1.95 0.05128 . ## QueenslandTRUE -0.10059 0.05747 -1.75 0.08037 . ## CherbourgTRUE 0.11348 0.03248 3.49 0.00050 *** ## age -0.00375 0.00112 -3.35 0.00083 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.388 on 1038 degrees of freedom ## (263 observations deleted due to missingness) ## Multiple R-squared: 0.381, Adjusted R-squared: 0.377 ## F-statistic: 91.4 on 7 and 1038 DF, p-value: &lt;2e-16 10.3 For the model from above, what are the minimum and maximum predicted probabilities of survival? ## Finding Min and Max results_pred &lt;- predict(lpm_2) max(results_pred) ## [1] 1.13 min(results_pred) ## [1] -0.18 10.4 What is the name, age, gender and passenger class of the person with the lowest probability of surviving? ## Extract the &quot;ID&quot; of individual with minimum survival probability id &lt;- names(results_pred[results_pred == min(results_pred)]) ## Look up the person from the original data dta$name[as.numeric(id)] # name ## [1] &quot;Connors, Mr. Patrick&quot; dta$age[as.numeric(id)] # age ## [1] 70.5 dta$sex[as.numeric(id)] # gender ## [1] &quot;male&quot; dta$pclass[as.numeric(id)] # passenger class ## [1] 3 10.5 What is the name of the person with the highest probability of surviving? ## Same as above but now for min id_1 &lt;- names(results_pred[results_pred == max(results_pred)]) dta$name[as.numeric(id_1)] # name ## [1] &quot;Hippach, Miss. Jean Gertrude&quot; dta$age[as.numeric(id_1)] # age ## [1] 16 dta$sex[as.numeric(id_1)] # gender ## [1] &quot;female&quot; dta$pclass[as.numeric(id_1)] # class ## [1] 1 10.6 Estimate a probit model where survival is a function of (only) passenger class. Treat passenger class as a nominal variable. Compare statistical significance to a similar LPM model. Is there an easy way to interpret the coefficients? ## Probit Model prob_1 &lt;- glm(survived ~ pclass_2 + pclass_3, data = dta, family = binomial(link = &quot;probit&quot;)) summary(prob_1) ## ## Call: ## glm(formula = survived ~ pclass_2 + pclass_3, family = binomial(link = &quot;probit&quot;), ## data = dta) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.390 -0.768 -0.768 0.979 1.653 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.3034 0.0709 4.28 1.9e-05 *** ## pclass_2TRUE -0.4808 0.1038 -4.63 3.6e-06 *** ## pclass_3TRUE -0.9613 0.0873 -11.01 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1741.0 on 1308 degrees of freedom ## Residual deviance: 1613.3 on 1306 degrees of freedom ## AIC: 1619 ## ## Number of Fisher Scoring iterations: 4 ## Logit Model logit_1 &lt;- glm(survived ~ pclass_2 + pclass_3, data = dta, family = binomial(link = &quot;logit&quot;)) summary(logit_1) ## ## Call: ## glm(formula = survived ~ pclass_2 + pclass_3, family = binomial(link = &quot;logit&quot;), ## data = dta) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.390 -0.768 -0.768 0.979 1.653 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.486 0.115 4.24 2.2e-05 *** ## pclass_2TRUE -0.770 0.167 -4.61 4.0e-06 *** ## pclass_3TRUE -1.557 0.143 -10.86 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1741.0 on 1308 degrees of freedom ## Residual deviance: 1613.3 on 1306 degrees of freedom ## AIC: 1619 ## ## Number of Fisher Scoring iterations: 4 10.7 Estimate a probit model where survival is a function of passenger class (treated as a nominal variable) age, gender, child and embarkation location. What is the minimum and maximum fitted value? ## Probit Model prob_2 &lt;- glm(survived ~ pclass_2 + pclass_3 + age + female + child + Queensland + Cherbourg, data = dta, family = binomial(link = &quot;probit&quot;)) summary(prob_2) ## ## Call: ## glm(formula = survived ~ pclass_2 + pclass_3 + age + female + ## child + Queensland + Cherbourg, family = binomial(link = &quot;probit&quot;), ## data = dta) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.669 -0.696 -0.424 0.681 2.522 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.20231 0.21119 0.96 0.33810 ## pclass_2TRUE -0.58732 0.13771 -4.27 2e-05 *** ## pclass_3TRUE -1.11893 0.13324 -8.40 &lt; 2e-16 *** ## age -0.01496 0.00443 -3.38 0.00072 *** ## femaleTRUE 1.50184 0.09552 15.72 &lt; 2e-16 *** ## childTRUE 0.23096 0.16872 1.37 0.17103 ## QueenslandTRUE -0.38235 0.22862 -1.67 0.09444 . ## CherbourgTRUE 0.41236 0.12201 3.38 0.00073 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1414.6 on 1045 degrees of freedom ## Residual deviance: 967.1 on 1038 degrees of freedom ## (263 observations deleted due to missingness) ## AIC: 983.1 ## ## Number of Fisher Scoring iterations: 5 ## Logit Model logit_2 &lt;- glm(survived ~ pclass_2 + pclass_3 + age + female + child + Queensland + Cherbourg, data = dta, family = binomial(link = &quot;logit&quot;)) summary(logit_2) ## ## Call: ## glm(formula = survived ~ pclass_2 + pclass_3 + age + female + ## child + Queensland + Cherbourg, family = binomial(link = &quot;logit&quot;), ## data = dta) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.591 -0.687 -0.418 0.666 2.477 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.35441 0.36614 0.97 0.33305 ## pclass_2TRUE -0.98280 0.23931 -4.11 4e-05 *** ## pclass_3TRUE -1.97477 0.23630 -8.36 &lt; 2e-16 *** ## age -0.02580 0.00776 -3.32 0.00089 *** ## femaleTRUE 2.53863 0.17010 14.92 &lt; 2e-16 *** ## childTRUE 0.47857 0.29554 1.62 0.10538 ## QueenslandTRUE -0.65048 0.39384 -1.65 0.09861 . ## CherbourgTRUE 0.70367 0.21200 3.32 0.00090 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1414.6 on 1045 degrees of freedom ## Residual deviance: 964.5 on 1038 degrees of freedom ## (263 observations deleted due to missingness) ## AIC: 980.5 ## ## Number of Fisher Scoring iterations: 4 ## Compute predicted probabilities prob_pred &lt;- predict(prob_2, type = &quot;response&quot;) max(prob_pred) ## [1] 0.982 min(prob_pred) ## [1] 0.0093 ## For the logit model logit_pred &lt;- predict(logit_2, type = &quot;response&quot;) max(logit_pred) ## [1] 0.975 min(logit_pred) ## [1] 0.0165 10.8 What is the name of the person with the lowest probability of surviving? ## Finding the Name and other demographic factors id_2 &lt;- names(prob_pred[prob_pred == min(prob_pred)]) dta$name[as.numeric(id_2)] ## [1] &quot;Connors, Mr. Patrick&quot; dta$pclass[as.numeric(id_2)] ## [1] 3 dta$age[as.numeric(id_2)] ## [1] 70.5 dta$sex[as.numeric(id_2)] ## [1] &quot;male&quot; 10.9 For the above model, what is the effect of growing one year older (for an adult)? (Do this “manually”, using the observed-value, discrete difference method described in the book/lecture.) ## Generate &quot;P1&quot; - the predicted values at actual values of X p_1 &lt;- pnorm(coef(prob_2)[1] + coef(prob_2)[2]*dta$pclass_2 + coef(prob_2)[3]*dta$pclass_3 + coef(prob_2)[4]*dta$age + coef(prob_2)[5]*dta$female + coef(prob_2)[6]*dta$child + coef(prob_2)[7]*dta$Queensland + coef(prob_2)[8]*dta$Cherbourg) ## Generate &quot;P2&quot; - the predicted values with age increased by 1 p_2 &lt;- pnorm(coef(prob_2)[1] + coef(prob_2)[2] *dta$pclass_2 + coef(prob_2)[3]*dta$pclass_3 + coef(prob_2)[4]*(dta$age + 1) + coef(prob_2)[5]*dta$female + coef(prob_2)[6]*dta$child + coef(prob_2)[7]*dta$Queensland + coef(prob_2)[8]*dta$Cherbourg) ## Taking the Difference diff_age &lt;- p_2 - p_1 ## Finding the Mean Difference mean(diff_age, na.rm = T) ## [1] -0.00388 ## The Marginal Effects Way mean(dnorm(coef(prob_2)[1] + coef(prob_2)[2] *dta$pclass_2 + coef(prob_2)[3]*dta$pclass_3 + coef(prob_2)[4]*dta$age + coef(prob_2)[5]*dta$female + coef(prob_2)[6]*dta$child + coef(prob_2)[7]*dta$Queensland + coef(prob_2)[8]*dta$Cherbourg)*coef(prob_2)[4], na.rm=T) ## [1] -0.00389 10.10 Compare the probit effect of age to the LPM effect of age in part (b) They are basically the same. 10.11 What is the effect of the passenger class, female and child variables in the above probit model? Use the mfx package as described in the lecture. Compare the predicted effects of these variables in the probit model to the results in the LPM in part (b). You need only discuss one of these variables, but please note all of them as you do the work. ## Marginal Effects Approach ## Probt Model probitmfx(prob_2, data = dta, atmean = FALSE) ## Call: ## probitmfx(formula = prob_2, data = dta, atmean = FALSE) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## pclass_2TRUE -0.14472 0.03140 -4.61 4e-06 *** ## pclass_3TRUE -0.30698 0.03449 -8.90 &lt; 2e-16 *** ## age -0.00389 0.00114 -3.42 0.00062 *** ## femaleTRUE 0.48869 0.02805 17.42 &lt; 2e-16 *** ## childTRUE 0.06139 0.04568 1.34 0.17897 ## QueenslandTRUE -0.09540 0.05424 -1.76 0.07859 . ## CherbourgTRUE 0.11230 0.03427 3.28 0.00105 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## dF/dx is for discrete change for the following variables: ## ## [1] &quot;pclass_2TRUE&quot; &quot;pclass_3TRUE&quot; &quot;femaleTRUE&quot; &quot;childTRUE&quot; &quot;QueenslandTRUE&quot; &quot;CherbourgTRUE&quot; ## Logit Model logitmfx(logit_2, data = dta, atmean = FALSE) ## Call: ## logitmfx(formula = logit_2, data = dta, atmean = FALSE) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## pclass_2TRUE -0.13885 0.03133 -4.43 9.3e-06 *** ## pclass_3TRUE -0.31215 0.03488 -8.95 &lt; 2e-16 *** ## age -0.00382 0.00119 -3.21 0.0013 ** ## femaleTRUE 0.48638 0.02808 17.32 &lt; 2e-16 *** ## childTRUE 0.07302 0.04613 1.58 0.1134 ## QueenslandTRUE -0.09259 0.05336 -1.74 0.0827 . ## CherbourgTRUE 0.10949 0.03404 3.22 0.0013 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## dF/dx is for discrete change for the following variables: ## ## [1] &quot;pclass_2TRUE&quot; &quot;pclass_3TRUE&quot; &quot;femaleTRUE&quot; &quot;childTRUE&quot; &quot;QueenslandTRUE&quot; &quot;CherbourgTRUE&quot; 10.11.0.1 Extra Plots ## LPM Plot dta %&gt;% ggplot(aes(x = fare, y = survived)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;) + theme_bw() ## Probit Plot dta %&gt;% ggplot(aes(x = fare, y = survived)) + geom_point() + stat_smooth(method = &quot;glm&quot;, se = FALSE, color = &quot;black&quot;, method.args = list(family=binomial(link = &quot;probit&quot;)))+ theme_bw() ## Both dta %&gt;% ggplot(aes(x = fare, y = survived)) + geom_point() + stat_smooth(method = &quot;glm&quot;, se = FALSE, color = &quot;tomato&quot;, method.args = list(family=binomial(link = &quot;probit&quot;))) + stat_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;light blue&quot;) + theme_bw() + labs(y = &quot;Survived&quot;, x = &quot;Cost of Fare&quot;) "]]
